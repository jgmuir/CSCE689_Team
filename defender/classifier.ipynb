{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf4c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED IMPORTS\n",
    "import os                                               # Directory walking for file loading\n",
    "import math                                             # Logarithm function\n",
    "import pandas as pd                                     # Dataframe managment\n",
    "import pefile                                           # Header feature extraction\n",
    "import dis                                              # x86 disassembly\n",
    "import pickle                                           # Model saving\n",
    "from io import StringIO                                 # Reading disassembly result\n",
    "from sklearn.feature_selection import SelectFromModel   # Feature dimensionality reduction\n",
    "from sklearn.ensemble import RandomForestClassifier     # Random Forest Classifier\n",
    "from collections import Counter                         # Entropy calculations\n",
    "import numpy as np                                      # Matrix operations\n",
    "# EVALUATION IMPORTS\n",
    "from matplotlib import pyplot as plt                    # Output plotting\n",
    "import seaborn as sns                                   # Heatmap of confusion matrix\n",
    "from sklearn.metrics import confusion_matrix            # Confusion matrix\n",
    "from sklearn.metrics import classification_report       # Classification report\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "MAX_BYTES = 50000\n",
    "\n",
    "def entropy(data):\n",
    "    \"\"\"Calculate the entropy of a chunk of data.\"\"\"\n",
    "    if not data:\n",
    "        return 0.0\n",
    "    occurences = Counter(bytearray(data))\n",
    "    entropy = 0\n",
    "    for x in occurences.values():\n",
    "        p_x = float(x) / len(data)\n",
    "        entropy -= p_x * math.log(p_x, 2)\n",
    "    return entropy\n",
    "\n",
    "def get_header_features(pe, features):\n",
    "    # Get FILE_HEADER features\n",
    "    if (hasattr(pe, \"FILE_HEADER\")):\n",
    "        features[\"FILE_HEADER.MACHINE\"] = pe.FILE_HEADER.Machine\n",
    "        features[\"FILE_HEADER.SIZEOFOPTIONALHEADER\"] = pe.FILE_HEADER.SizeOfOptionalHeader\n",
    "        features[\"FILE_HEADER.CHARACTERISTICS\"] = pe.FILE_HEADER.Characteristics\n",
    "    else:\n",
    "        features[\"FILE_HEADER.MACHINE\"] = 0\n",
    "        features[\"FILE_HEADER.SIZEOFOPTIONALHEADER\"] = 0\n",
    "        features[\"FILE_HEADER.CHARACTERISTICS\"] = 0\n",
    "    # Get OPTIONAL_HEADER features\n",
    "    if (hasattr(pe, \"OPTIONAL_HEADER\")):\n",
    "        features[\"OPTIONAL_HEADER.IMAGEBASE\"] = pe.OPTIONAL_HEADER.ImageBase\n",
    "        features[\"OPTIONAL_HEADER.MAJOROPERATINGSYSTEM\"] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion\n",
    "        features[\"OPTIONAL_HEADER.MAJORSUBSYSTEMVERSION\"] = pe.OPTIONAL_HEADER.MajorSubsystemVersion\n",
    "        features[\"OPTIONAL_HEADER.DLLCHARACTERISTICS\"] = pe.OPTIONAL_HEADER.DllCharacteristics\n",
    "        features[\"OPTIONAL_HEADER.SUBSYSTEM\"] = pe.OPTIONAL_HEADER.Subsystem\n",
    "        entropies = []\n",
    "        for section in pe.sections:\n",
    "            entropies.append(section.get_entropy())\n",
    "        features[\"PE_SECTIONS.MAXENTROPY\"] = max(entropies)\n",
    "        features[\"PE_SECTIONS.MINENTROPY\"] = min(entropies)\n",
    "        features[\"PE_SECTIONS.MEANENTROPY\"] = sum(entropies) / len(entropies)\n",
    "    else:\n",
    "        features[\"OPTIONAL_HEADER.IMAGEBASE\"] = 0\n",
    "        features[\"OPTIONAL_HEADER.MAJOROPERATINGSYSTEM\"] = 0\n",
    "        features[\"OPTIONAL_HEADER.MAJORSUBSYSTEMVERSION\"] = 0\n",
    "        features[\"OPTIONAL_HEADER.DLLCHARACTERISTICS\"] = 0\n",
    "        features[\"OPTIONAL_HEADER.SUBSYSTEM\"] = 0\n",
    "        features[\"PE_SECTIONS.MAXENTROPY\"] = 0\n",
    "        features[\"PE_SECTIONS.MINENTROPY\"] = 0\n",
    "        features[\"PE_SECTIONS.MEANENTROPY\"] = 0\n",
    "    # Get DIRECTORY_ENTRY_RESOURCE features\n",
    "    if (hasattr(pe, \"DIRECTORY_ENTRY_RESOURCE\")):\n",
    "        # Find all resources in the PE and calculate their entropy\n",
    "        entropies = []\n",
    "        for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:\n",
    "            if resource_type.name is not None:\n",
    "                name = str(resource_type.name)\n",
    "            else:\n",
    "                name = str(pefile.RESOURCE_TYPE.get(resource_type.struct.Id))\n",
    "            if name is None:\n",
    "                name = str(resource_type.struct.Id)\n",
    "            if hasattr(resource_type, 'directory'):\n",
    "                for resource_id in resource_type.directory.entries:\n",
    "                    if hasattr(resource_id, 'directory'):\n",
    "                        for resource_lang in resource_id.directory.entries:\n",
    "                            if hasattr(resource_lang, \"data\"):\n",
    "                                try:\n",
    "                                    data = pe.get_data(resource_lang.data.struct.OffsetToData, resource_lang.data.struct.Size)\n",
    "                                    entropies.append(entropy(data))\n",
    "                                except:\n",
    "                                    entropies.append(0)\n",
    "        if len(entropies) > 0:\n",
    "            features[\"RESOURCES.MAXENTROPY\"] = max(entropies)\n",
    "            features[\"RESOURCES.MINENTROPY\"] = min(entropies)\n",
    "        else:\n",
    "            features[\"RESOURCES.MAXENTROPY\"] = 0\n",
    "            features[\"RESOURCES.MINENTROPY\"] = 0\n",
    "    else:\n",
    "        features[\"RESOURCES.MAXENTROPY\"] = 0\n",
    "        features[\"RESOURCES.MINENTROPY\"] = 0   \n",
    "    # Get VS_VERSIONINFO feature\n",
    "    if (hasattr(pe, \"VS_VERSIONINFO\")):\n",
    "        features[\"VS_VERSIONINFO.Length\"] = pe.VS_VERSIONINFO[0].Length\n",
    "    else:\n",
    "        features[\"VS_VERSIONINFO.Length\"] = 0\n",
    "    # Return the final header features\n",
    "    return features\n",
    "\n",
    "def get_byte_file(pe):\n",
    "    try:\n",
    "        global MAX_BYTES\n",
    "        if pe.OPTIONAL_HEADER.SizeOfCode > MAX_BYTES:\n",
    "            byte_file = pe.get_data(pe.OPTIONAL_HEADER.BaseOfCode, MAX_BYTES)\n",
    "        else:\n",
    "            byte_file = pe.get_data(pe.OPTIONAL_HEADER.BaseOfCode, pe.OPTIONAL_HEADER.SizeOfCode)\n",
    "        return byte_file\n",
    "    except:\n",
    "        return bytearray()\n",
    "    \n",
    "def get_training_byte_features(byte_files, classifications):\n",
    "    # Initialize the set of unique bi-gram byte features\n",
    "    all_unique_bi_grams = set()\n",
    "    each_file_bi_grams = []\n",
    "    # Iterate over the byte files\n",
    "    for sample, byte_file in byte_files.items():\n",
    "        print(\"Collecting bi-gram byte features for sample \" + str(sample))\n",
    "        # Identify new unique bi-grams in the current byte file\n",
    "        cur_unique_bi_grams = set()\n",
    "        prev_byte = None\n",
    "        for i, byte in enumerate(byte_file):\n",
    "            # Creating the bi-gram if enough history exists\n",
    "            if prev_byte != None:\n",
    "                bi_gram = hex(prev_byte) + \" \" + hex(byte)\n",
    "                # If the current bi-gram has not been seen in the current file\n",
    "                if not bi_gram in cur_unique_bi_grams:\n",
    "                    cur_unique_bi_grams.add(bi_gram)\n",
    "                # If the current bi-gram has not been seen in any file\n",
    "                if not bi_gram in all_unique_bi_grams:\n",
    "                    all_unique_bi_grams.add(bi_gram)\n",
    "            # Moving the sliding window\n",
    "            prev_byte = byte\n",
    "        # Add the set of unique bi-grams for the current file to the list\n",
    "        each_file_bi_grams.append(cur_unique_bi_grams)\n",
    "    # Initialize the bi-gram byte feature matrix\n",
    "    num_rows = len(byte_files)\n",
    "    num_cols = len(list(all_unique_bi_grams))\n",
    "    byte_bi_gram_features_list = [[0]*num_cols for i in range(num_rows)]          \n",
    "    # One-hot-encoding every sample with the combination of all encountered features\n",
    "    for row, file_bi_grams in enumerate(each_file_bi_grams):\n",
    "        print(\"One-hot-encoding bi-gram byte features for sample \" + str(list(byte_files)[row]))\n",
    "        for col, bi_gram in enumerate(all_unique_bi_grams):\n",
    "            if bi_gram in file_bi_grams:\n",
    "                byte_bi_gram_features_list[row][col] = 1\n",
    "    # Creating the feature selector model for top 200 features\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(n_estimators=1000), max_features=200)\n",
    "    # Selecting top 200 bi-gram byte features\n",
    "    print(\"Selecting top 200 bi-gram byte features\")\n",
    "    selector.fit(np.array(byte_bi_gram_features_list), list(classifications))\n",
    "    selections = selector.get_support()\n",
    "    # Copy the selected features to another matrix\n",
    "    print(\"Copying selected bi-gram byte features\")\n",
    "    selected_byte_bi_gram_features_dict = {}\n",
    "    selected_indicies = [i for i, e in enumerate(selections) if e == True]\n",
    "    for index in selected_indicies:\n",
    "        selected_byte_bi_gram_features_dict[list(all_unique_bi_grams)[index]] = [row[index] for row in byte_bi_gram_features_list]\n",
    "    print(\"Converting selected bi-gram byte features to DataFrame\")\n",
    "    byte_bi_gram_features = pd.DataFrame(selected_byte_bi_gram_features_dict)\n",
    "    return byte_bi_gram_features\n",
    "\n",
    "def get_validation_byte_features(byte_files, selected_byte_features):\n",
    "    # Initialize the set of unique bi-gram byte features\n",
    "    each_file_bi_grams = []\n",
    "    # Iterate over the byte files\n",
    "    for sample, byte_file in byte_files.items():\n",
    "        print(\"Collecting bi-gram byte features for sample \" + str(sample))\n",
    "        # Identify new unique bi-grams in the current byte file\n",
    "        cur_unique_bi_grams = set()\n",
    "        prev_byte = None\n",
    "        for i, byte in enumerate(byte_file):\n",
    "            # Creating the bi-gram if enough history exists\n",
    "            if prev_byte != None:\n",
    "                bi_gram = hex(prev_byte) + \" \" + hex(byte)\n",
    "                # If the current bi-gram has not been seen in the current file\n",
    "                if not bi_gram in cur_unique_bi_grams:\n",
    "                    cur_unique_bi_grams.add(bi_gram)\n",
    "            # Moving the sliding window\n",
    "            prev_byte = byte\n",
    "        # Add the set of unique bi-grams for the current file to the list\n",
    "        each_file_bi_grams.append(cur_unique_bi_grams)\n",
    "    # Initialize the bi-gram byte feature matrix\n",
    "    num_rows = len(byte_files)\n",
    "    num_cols = len(selected_byte_features)\n",
    "    byte_bi_gram_features_list = [[0]*num_cols for i in range(num_rows)]             \n",
    "    # One-hot-encoding every sample with the combination of all encountered features\n",
    "    for row, file_bi_grams in enumerate(each_file_bi_grams):\n",
    "        print(\"One-hot-encoding bi-gram byte features for sample \" + str(list(byte_files)[row]))\n",
    "        for col, bi_gram in enumerate(selected_byte_features):\n",
    "            if bi_gram in file_bi_grams:\n",
    "                byte_bi_gram_features_list[row][col] = 1\n",
    "    # Convert the bi-gram byte features into a dataframe object\n",
    "    print(\"Converting bi-gram byte features to DataFrame\")\n",
    "    byte_bi_gram_features = pd.DataFrame(byte_bi_gram_features_list, columns=selected_byte_features)\n",
    "    return byte_bi_gram_features\n",
    "\n",
    "def get_classification_byte_features(byte_file, selected_byte_features):\n",
    "    # Identify new unique bi-grams in the byte file\n",
    "    unique_bi_grams = set()\n",
    "    prev_byte = None\n",
    "    for i, byte in enumerate(byte_file):\n",
    "        # Creating the bi-gram if enough history exists\n",
    "        if prev_byte != None:\n",
    "            bi_gram = hex(prev_byte) + \" \" + hex(byte)\n",
    "            # If the current bi-gram has not been seen in the current file\n",
    "            if not bi_gram in unique_bi_grams:\n",
    "                unique_bi_grams.add(bi_gram)\n",
    "        # Moving the sliding window\n",
    "        prev_byte = byte\n",
    "    # Initialize the bi-gram byte feature matrix\n",
    "    num_cols = len(selected_byte_features)\n",
    "    byte_bi_gram_features_list = [0]*num_cols            \n",
    "    # One-hot-encoding the sample with the combination of all encountered features\n",
    "    for col, bi_gram in enumerate(selected_byte_features):\n",
    "        if bi_gram in list(unique_bi_grams):\n",
    "            byte_bi_gram_features_list[col] = 1\n",
    "    # Convert the bi-gram byte features into a dataframe object\n",
    "    byte_bi_gram_features = pd.DataFrame([byte_bi_gram_features_list], columns=selected_byte_features)\n",
    "    return byte_bi_gram_features\n",
    "    \n",
    "def get_asm_file(pe):\n",
    "    # Try reading the instructions as bytes\n",
    "    try:\n",
    "        global MAX_BYTES\n",
    "        if pe.OPTIONAL_HEADER.SizeOfCode > MAX_BYTES:\n",
    "            file_bytes = pe.get_data(pe.OPTIONAL_HEADER.BaseOfCode, MAX_BYTES)\n",
    "        else:\n",
    "            file_bytes = pe.get_data(pe.OPTIONAL_HEADER.BaseOfCode, pe.OPTIONAL_HEADER.SizeOfCode)\n",
    "    except:\n",
    "        # If fails return list of no instructions found\n",
    "        return []\n",
    "    # Do weird try/catch since dis.dis is broken\n",
    "    out = StringIO()\n",
    "    try:\n",
    "        dis.dis(x=file_bytes, file=out)\n",
    "    except:\n",
    "        pass\n",
    "    # Parse the disasembled instructions and create a list of OPCODES\n",
    "    lines = out.getvalue().split('\\n')\n",
    "    lines = lines[0:len(lines)-1]\n",
    "    asm_file = [line.strip().strip('>>').strip().split(' ')[1] for line in lines if line.strip().strip('>>').strip()]\n",
    "    return asm_file\n",
    "    \n",
    "def get_training_asm_features(asm_files, classifications):\n",
    "    # Initialize the set of unique bi-gram and tri-gram OPCODE features\n",
    "    all_unique_bi_grams = set()\n",
    "    all_unique_tri_grams = set()\n",
    "    each_file_bi_grams = []\n",
    "    each_file_tri_grams = []\n",
    "    # Iterate over the ASM files\n",
    "    for sample, asm_file in asm_files.items():\n",
    "        print(\"Collecting bi-gram and tri-gram OPCODE features for sample \" + str(sample))\n",
    "        # Identify new unique bi-grams and tri-grams in the current ASM file\n",
    "        cur_unique_bi_grams = set()\n",
    "        cur_unique_tri_grams = set()\n",
    "        prev_opcode1 = None\n",
    "        prev_opcode2 = None\n",
    "        for i, opcode in enumerate(asm_file):\n",
    "            # Creating the bi-gram if enough history exists\n",
    "            if prev_opcode1 != None:\n",
    "                bi_gram = prev_opcode1 + \" \" + opcode\n",
    "                # If the current bi-gram has not been seen in the current file\n",
    "                if not bi_gram in cur_unique_bi_grams:\n",
    "                    cur_unique_bi_grams.add(bi_gram)\n",
    "                # If the current bi-gram has not been seen in any file\n",
    "                if not bi_gram in all_unique_bi_grams:\n",
    "                    all_unique_bi_grams.add(bi_gram)\n",
    "            # Creating the tri-gram if enough history exists\n",
    "            if prev_opcode2 != None:\n",
    "                tri_gram = prev_opcode2 + \" \" + prev_opcode1 + \" \" + opcode\n",
    "                # If the current tri-gram has not been seen in the current file\n",
    "                if not tri_gram in cur_unique_tri_grams:\n",
    "                    cur_unique_tri_grams.add(tri_gram)\n",
    "                # If the current tri-gram has not been seen in any file\n",
    "                if not tri_gram in all_unique_tri_grams:\n",
    "                    all_unique_tri_grams.add(tri_gram)\n",
    "            # Moving the sliding window\n",
    "            prev_opcode2 = prev_opcode1\n",
    "            prev_opcode1 = opcode\n",
    "        # Add the set of unique bi-grams and tri-grams for the current file to the list\n",
    "        each_file_bi_grams.append(cur_unique_bi_grams)\n",
    "        each_file_tri_grams.append(cur_unique_tri_grams)\n",
    "    # Initialize the bi-gram OPCODE feature matrix\n",
    "    num_rows = len(asm_files)\n",
    "    num_cols1 = len(list(all_unique_bi_grams))\n",
    "    num_cols2 = len(list(all_unique_tri_grams))\n",
    "    opcode_bi_gram_features_list = [[0]*num_cols1 for i in range(num_rows)]\n",
    "    opcode_tri_gram_features_list = [[0]*num_cols2 for i in range(num_rows)]             \n",
    "    # One-hot-encoding every sample with the combination of all encountered features\n",
    "    for row, file_bi_grams in enumerate(each_file_bi_grams):\n",
    "        print(\"One-hot-encoding bi-gram OPCODE features for sample \" + str(list(asm_files)[row]))\n",
    "        for col, bi_gram in enumerate(all_unique_bi_grams):\n",
    "            if bi_gram in file_bi_grams:\n",
    "                opcode_bi_gram_features_list[row][col] = 1\n",
    "    for row, file_tri_grams in enumerate(each_file_tri_grams):\n",
    "        print(\"One-hot-encoding tri-gram OPCODE features for sample \" + str(list(asm_files)[row]))\n",
    "        for col, tri_gram in enumerate(all_unique_tri_grams):\n",
    "            if tri_gram in file_tri_grams:\n",
    "                opcode_tri_gram_features_list[row][col] = 1\n",
    "    # Creating the feature selector model for top 100 features\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(n_estimators=1000), max_features=100)\n",
    "    # Selecting top 100 bi-gram opcode features\n",
    "    print(\"Selecting top 100 bi-gram OPCODE features\")\n",
    "    selector.fit(np.array(opcode_bi_gram_features_list), list(classifications))\n",
    "    selections = selector.get_support()\n",
    "    # Copying the selected bi-gram features to another matrix\n",
    "    print(\"Copying selected bi-gram OPCODE features\")\n",
    "    selected_opcode_bi_gram_features_dict = {}\n",
    "    selected_indicies = [i for i, e in enumerate(selections) if e == True]\n",
    "    for index in selected_indicies:\n",
    "        selected_opcode_bi_gram_features_dict[list(all_unique_bi_grams)[index]] = [row[index] for row in opcode_bi_gram_features_list]\n",
    "    print(\"Converting selected bi-gram byte features to DataFrame\")\n",
    "    opcode_bi_gram_features = pd.DataFrame(selected_opcode_bi_gram_features_dict)\n",
    "    # Selecting top 100 tri-gram opcode features\n",
    "    print(\"Selecting top 100 tri-gram OPCODE features\")\n",
    "    selector.fit(np.array(opcode_tri_gram_features_list), list(classifications))\n",
    "    selections = selector.get_support()\n",
    "    # Copy the selected features to another matrix\n",
    "    print(\"Copying selected tri-gram OPCODE features\")\n",
    "    selected_opcode_tri_gram_features_dict = {}\n",
    "    selected_indicies = [i for i, e in enumerate(selections) if e == True]\n",
    "    for index in selected_indicies:\n",
    "        selected_opcode_tri_gram_features_dict[list(all_unique_tri_grams)[index]] = [row[index] for row in opcode_tri_gram_features_list]\n",
    "    print(\"Converting selected bi-gram byte features to DataFrame\")\n",
    "    opcode_tri_gram_features = pd.DataFrame(selected_opcode_tri_gram_features_dict)\n",
    "    return opcode_bi_gram_features, opcode_tri_gram_features\n",
    "\n",
    "def get_validation_asm_features(asm_files, selected_opcode_features_1, selected_opcode_features_2):\n",
    "    # Initialize the set of unique bi-gram and tri-gram OPCODE features\n",
    "    each_file_bi_grams = []\n",
    "    each_file_tri_grams = []\n",
    "    # Iterate over the ASM files\n",
    "    for sample, asm_file in asm_files.items():\n",
    "        print(\"Collecting bi-gram and tri-gram OPCODE features for sample \" + str(sample))\n",
    "        # Identify new unique bi-grams and tri-grams in the current ASM file\n",
    "        cur_unique_bi_grams = set()\n",
    "        cur_unique_tri_grams = set()\n",
    "        prev_opcode1 = None\n",
    "        prev_opcode2 = None\n",
    "        for i, opcode in enumerate(asm_file):\n",
    "            # Creating the bi-gram if enough history exists\n",
    "            if prev_opcode1 != None:\n",
    "                bi_gram = prev_opcode1 + \" \" + opcode\n",
    "                # If the current bi-gram has not been seen in the current file\n",
    "                if not bi_gram in cur_unique_bi_grams:\n",
    "                    cur_unique_bi_grams.add(bi_gram)\n",
    "            # Creating the tri-gram if enough history exists\n",
    "            if prev_opcode2 != None:\n",
    "                tri_gram = prev_opcode2 + \" \" + prev_opcode1 + \" \" + opcode\n",
    "                # If the current tri-gram has not been seen in the current file\n",
    "                if not tri_gram in cur_unique_tri_grams:\n",
    "                    cur_unique_tri_grams.add(tri_gram)\n",
    "            # Moving the sliding window\n",
    "            prev_opcode2 = prev_opcode1\n",
    "            prev_opcode1 = opcode\n",
    "        # Add the set of unique bi-grams and tri-grams for the current file to the list\n",
    "        each_file_bi_grams.append(cur_unique_bi_grams)\n",
    "        each_file_tri_grams.append(cur_unique_tri_grams)\n",
    "    # Initialize the bi-gram OPCODE feature matrix\n",
    "    num_rows = len(asm_files)\n",
    "    num_cols1 = len(selected_opcode_features_1)\n",
    "    num_cols2 = len(selected_opcode_features_2)\n",
    "    opcode_bi_gram_features_list = [[0]*num_cols1 for i in range(num_rows)]\n",
    "    opcode_tri_gram_features_list = [[0]*num_cols2 for i in range(num_rows)]              \n",
    "    # One-hot-encoding every sample with the combination of all encountered features\n",
    "    for row, file_bi_grams in enumerate(each_file_bi_grams):\n",
    "        print(\"One-hot-encoding bi-gram OPCODE features for sample \" + str(list(asm_files)[row]))\n",
    "        for col, bi_gram in enumerate(selected_opcode_features_1):\n",
    "            if bi_gram in file_bi_grams:\n",
    "                opcode_bi_gram_features_list[row][col] = 1\n",
    "    for row, file_tri_grams in enumerate(each_file_tri_grams):\n",
    "        print(\"One-hot-encoding tri-gram OPCODE features for sample \" + str(list(asm_files)[row]))\n",
    "        for col, tri_gram in enumerate(selected_opcode_features_2):\n",
    "            if tri_gram in file_tri_grams:\n",
    "                opcode_tri_gram_features_list[row][col] = 1\n",
    "    # Convert the bi-gram OPCODE features into a dataframe object\n",
    "    print(\"Converting bi-gram OPCODE features to DataFrame\")\n",
    "    opcode_bi_gram_features = pd.DataFrame(opcode_bi_gram_features_list, columns=selected_opcode_features_1)\n",
    "    # Convert the tri-gram OPCODE features into a dataframe object\n",
    "    print(\"Converting tri-gram OPCODE features to DataFrame\")\n",
    "    opcode_tri_gram_features = pd.DataFrame(opcode_tri_gram_features_list, columns=selected_opcode_features_2)\n",
    "    return opcode_bi_gram_features, opcode_tri_gram_features\n",
    "\n",
    "def get_classification_asm_features(asm_file, selected_opcode_features_1, selected_opcode_features_2):\n",
    "    # Identify unique bi-grams and tri-grams\n",
    "    unique_bi_grams = set()\n",
    "    unique_tri_grams = set()\n",
    "    prev_opcode1 = None\n",
    "    prev_opcode2 = None\n",
    "    for i, opcode in enumerate(asm_file):\n",
    "        # Creating the bi-gram if enough history exists\n",
    "        if prev_opcode1 != None:\n",
    "            bi_gram = prev_opcode1 + \" \" + opcode\n",
    "            # If the current bi-gram has not been seen\n",
    "            if not bi_gram in unique_bi_grams:\n",
    "                unique_bi_grams.add(bi_gram)\n",
    "        # Creating the tri-gram if enough history exists\n",
    "        if prev_opcode2 != None:\n",
    "            tri_gram = prev_opcode2 + \" \" + prev_opcode1 + \" \" + opcode\n",
    "            # If the current tri-gram has not been seen\n",
    "            if not tri_gram in unique_tri_grams:\n",
    "                unique_tri_grams.add(tri_gram)\n",
    "        # Moving the sliding window\n",
    "        prev_opcode2 = prev_opcode1\n",
    "        prev_opcode1 = opcode\n",
    "    # Initialize the bi-gram OPCODE feature matrix\n",
    "    num_cols1 = len(selected_opcode_features_1)\n",
    "    num_cols2 = len(selected_opcode_features_2)\n",
    "    opcode_bi_gram_features_list = [0]*num_cols1\n",
    "    opcode_tri_gram_features_list = [0]*num_cols2            \n",
    "    # One-hot-encoding the sample with the combination of all encountered features\n",
    "    for col, bi_gram in enumerate(selected_opcode_features_1):\n",
    "        if bi_gram in list(unique_bi_grams):\n",
    "            opcode_bi_gram_features_list[col] = 1\n",
    "    for col, tri_gram in enumerate(selected_opcode_features_2):\n",
    "        if tri_gram in list(unique_tri_grams):\n",
    "            opcode_tri_gram_features_list[col] = 1\n",
    "    # Convert the bi-gram OPCODE features into a dataframe object\n",
    "    opcode_bi_gram_features = pd.DataFrame([opcode_bi_gram_features_list], columns=selected_opcode_features_1)\n",
    "    # Convert the tri-gram OPCODE features into a dataframe object\n",
    "    opcode_tri_gram_features = pd.DataFrame([opcode_tri_gram_features_list], columns=selected_opcode_features_2)\n",
    "    return opcode_bi_gram_features, opcode_tri_gram_features\n",
    "\n",
    "def create_training_feature_vectors(sample_dir):\n",
    "    # Creating initial header feature dataframe\n",
    "    header_feature_df = pd.DataFrame()\n",
    "    # Creating structure to store all byte and ASM files\n",
    "    byte_files = {}\n",
    "    asm_files = {}\n",
    "    # Iterating through all samples in the samples directory\n",
    "    for root, dirs, files in os.walk(sample_dir):\n",
    "        for file in files:\n",
    "            sample = os.path.join(root,file)\n",
    "            print(\"Processing sample \" + str(sample))\n",
    "            # Creating initial entry for the current sample\n",
    "            header_features = {}\n",
    "            header_features[\"SAMPLE\"] = (sample)\n",
    "            if (\"malicious\" in root):\n",
    "                header_features[\"CLASSIFICATION\"] = 1 \n",
    "            else:\n",
    "                header_features[\"CLASSIFICATION\"] = 0\n",
    "            # Try to process the sample as a PE file\n",
    "            try:\n",
    "                pe = pefile.PE(sample)\n",
    "            except:\n",
    "                # Sample not a PE file so put empty features and add it to the feature dataframe\n",
    "                print(\"Sample \" + str(sample) + \" is not a PE file, creating empty feature vector\")\n",
    "                header_feature_df = header_feature_df.append(header_features, ignore_index=True)\n",
    "                byte_files[sample] = bytearray()\n",
    "                asm_files[sample] = []\n",
    "                continue\n",
    "            # Collecting PE header features from the current sample\n",
    "            print(\"Collecting header features for sample \" + str(sample))\n",
    "            header_features = get_header_features(pe, header_features)\n",
    "            header_feature_df = header_feature_df.append(header_features, ignore_index=True)\n",
    "            # Gathering byte file for the current sample\n",
    "            print(\"Gathering byte file for sample \" + str(sample))\n",
    "            byte_files[sample] = get_byte_file(pe)\n",
    "            # Gathering ASM file for the current sample\n",
    "            print(\"Gathering ASM file for sample \" + str(sample))\n",
    "            asm_files[sample] = get_asm_file(pe)\n",
    "    # Creating byte-based feature matrix\n",
    "    print(\"Creating byte-based feature matrix\")\n",
    "    byte_bi_gram_features = get_training_byte_features(byte_files, list(header_feature_df[\"CLASSIFICATION\"]))\n",
    "    # Creating opcode-based feature matrix\n",
    "    print(\"Creating opcode-based feature matrix\")\n",
    "    opcode_bi_gram_features, opcode_tri_gram_features = get_training_asm_features(asm_files, list(header_feature_df[\"CLASSIFICATION\"]))\n",
    "    # Creating final dataset with full feature matrix\n",
    "    final_feature_df = pd.concat([header_feature_df, byte_bi_gram_features, opcode_bi_gram_features, opcode_tri_gram_features])\n",
    "    # Fill empty spaces in the dataframe with 0s\n",
    "    final_feature_df = final_feature_df.fillna(0)\n",
    "    return final_feature_df, byte_bi_gram_features.columns, opcode_bi_gram_features.columns, opcode_tri_gram_features.columns\n",
    "\n",
    "def create_validation_feature_vectors(sample_dir, selected_byte_features, selected_opcode_features_1, selected_opcode_features_2):\n",
    "    # Creating initial header feature dataframe\n",
    "    header_feature_df = pd.DataFrame()\n",
    "    # Creating structure to store all byte and ASM files\n",
    "    byte_files = {}\n",
    "    asm_files = {}\n",
    "    # Iterating through all samples in the samples directory\n",
    "    for root, dirs, files in os.walk(sample_dir):\n",
    "        for file in files:\n",
    "            sample = os.path.join(root,file)\n",
    "            print(\"Processing sample \" + str(sample))\n",
    "            # Creating initial entry for the current sample\n",
    "            header_features = {}\n",
    "            header_features[\"SAMPLE\"] = (sample)\n",
    "            if (\"malicious\" in root):\n",
    "                header_features[\"CLASSIFICATION\"] = 1 \n",
    "            else:\n",
    "                header_features[\"CLASSIFICATION\"] = 0\n",
    "            # Try to process the sample as a PE file\n",
    "            try:\n",
    "                pe = pefile.PE(sample)\n",
    "            except:\n",
    "                # Sample not a PE file so put empty features and add it to the feature dataframe\n",
    "                print(\"Sample \" + str(sample) + \" is not a PE file, creating empty feature vector\")\n",
    "                header_feature_df = header_feature_df.append(header_features, ignore_index=True)\n",
    "                byte_files[sample] = bytearray()\n",
    "                asm_files[sample] = []\n",
    "                continue\n",
    "            # Collecting PE header features from the current sample\n",
    "            print(\"Collecting header features for sample \" + str(sample))\n",
    "            header_features = get_header_features(pe, header_features)\n",
    "            header_feature_df = header_feature_df.append(header_features, ignore_index=True)\n",
    "            # Gathering byte file for the current sample\n",
    "            print(\"Gathering byte file for sample \" + str(sample))\n",
    "            byte_files[sample] = get_byte_file(pe)\n",
    "            # Gathering ASM file for the current sample\n",
    "            print(\"Gathering ASM file for sample \" + str(sample))\n",
    "            asm_files[sample] = get_asm_file(pe)\n",
    "    # Creating byte-based feature matrix\n",
    "    print(\"Creating byte-based feature matrix\")\n",
    "    byte_bi_gram_features = get_validation_byte_features(byte_files, selected_byte_features)\n",
    "    # Creating opcode-based feature matrix\n",
    "    print(\"Creating opcode-based feature matrix\")\n",
    "    opcode_bi_gram_features, opcode_tri_gram_features = get_validation_asm_features(asm_files, selected_opcode_features_1, selected_opcode_features_2)\n",
    "    # Creating final dataset with full feature matrix\n",
    "    final_feature_df = pd.concat([header_feature_df, byte_bi_gram_features, opcode_bi_gram_features, opcode_tri_gram_features])\n",
    "    # Fill empty spaces in the dataframe with 0s\n",
    "    final_feature_df = final_feature_df.fillna(0)\n",
    "    return final_feature_df\n",
    "\n",
    "   \n",
    "def parse_selected_features(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    bi_gram_byte_features = []\n",
    "    bi_gram_opcode_features = []\n",
    "    tri_gram_opcode_features = []\n",
    "\n",
    "    current_section = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if line == \"BI-GRAM BYTE FEATURES\":\n",
    "            current_section = \"bi_gram_byte\"\n",
    "        elif line == \"BI-GRAM OPCODE FEATURES\":\n",
    "            current_section = \"bi_gram_opcode\"\n",
    "        elif line == \"TRI-GRAM OPCODE FEATURES\":\n",
    "            current_section = \"tri_gram_opcode\"\n",
    "        else:\n",
    "            if current_section == \"bi_gram_byte\":\n",
    "                bi_gram_byte_features.append(line)\n",
    "            elif current_section == \"bi_gram_opcode\":\n",
    "                bi_gram_opcode_features.append(line)\n",
    "            elif current_section == \"tri_gram_opcode\":\n",
    "                tri_gram_opcode_features.append(line)\n",
    "    print(len(bi_gram_byte_features), len(bi_gram_opcode_features), len(tri_gram_opcode_features))\n",
    "    return bi_gram_byte_features, bi_gram_opcode_features, tri_gram_opcode_features\n",
    "\n",
    "\n",
    "\n",
    "def create_classification_feature_vector(pe, selected_feature_path):\n",
    "    selected_byte_features, selected_opcode_features_1, selected_opcode_features_2=parse_selected_features(selected_feature_path)\n",
    "    # Collecting PE header features from the sample\n",
    "    header_features= pd.DataFrame()\n",
    "    header_features = get_header_features(pe, header_features)\n",
    "    # Gathering byte file for the sample\n",
    "    byte_file = get_byte_file(pe)\n",
    "    # Gathering ASM file for the sample\n",
    "    asm_file = get_asm_file(pe)\n",
    "    # Creating byte-based features\n",
    "    byte_bi_gram_features = get_classification_byte_features(byte_file, selected_byte_features)\n",
    "    # Creating opcode-based features\n",
    "    opcode_bi_gram_features, opcode_tri_gram_features = get_classification_asm_features(asm_file, selected_opcode_features_1, selected_opcode_features_2)\n",
    "    # Creating final feature vector\n",
    "    features = pd.concat([header_features, byte_bi_gram_features, opcode_bi_gram_features, opcode_tri_gram_features],ignore_index=True)\n",
    "    # Fill empty spaces in the dataframe with 0s\n",
    "    print(features.shape)\n",
    "    features = features.fillna(0)\n",
    "    return features\n",
    "\n",
    "def create_feature_vector(file_obj):\n",
    "    # Creating initial feature dataframe\n",
    "    feature_df = pd.DataFrame()\n",
    "\n",
    "    # Collecting PE Header features from the input file\n",
    "    features = {}\n",
    "    try:\n",
    "        pe = pefile.PE(data=file_obj.read())\n",
    "\n",
    "        file_header = getattr(pe, \"FILE_HEADER\", None)\n",
    "        features[\"FILE_HEADER.MACHINE\"] = file_header.Machine if file_header else 0\n",
    "        features[\"FILE_HEADER.SIZEOFOPTIONALHEADER\"] = file_header.SizeOfOptionalHeader if file_header else 0\n",
    "        features[\"FILE_HEADER.CHARACTERISTICS\"] = file_header.Characteristics if file_header else 0\n",
    "\n",
    "        entropies = []\n",
    "        byte_files = []\n",
    "        if (hasattr(pe, \"OPTIONAL_HEADER\")):\n",
    "            features[\"OPTIONAL_HEADER.IMAGEBASE\"] = pe.OPTIONAL_HEADER.ImageBase\n",
    "            features[\"OPTIONAL_HEADER.MAJOROPERATINGSYSTEM\"] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion\n",
    "            features[\"OPTIONAL_HEADER.MAJORSUBSYSTEMVERSION\"] = pe.OPTIONAL_HEADER.MajorSubsystemVersion\n",
    "            features[\"OPTIONAL_HEADER.DLLCHARACTERISTICS\"] = pe.OPTIONAL_HEADER.DllCharacteristics\n",
    "            features[\"OPTIONAL_HEADER.SUBSYSTEM\"] = pe.OPTIONAL_HEADER.Subsystem\n",
    "            for section in pe.sections:\n",
    "                entropies.append(section.get_entropy())\n",
    "            for directory in pe.OPTIONAL_HEADER.DATA_DIRECTORY:\n",
    "                features[\"DATA_DIRECTORY.\"+str(directory.name)] = 1 if ((directory.VirtualAddress != 0) and (directory.Size != 0)) else 0\n",
    "            byte_files.append(pe.get_data(pe.OPTIONAL_HEADER.BaseOfCode, pe.OPTIONAL_HEADER.SizeOfCode))\n",
    "            # TODO: Get ASM file here\n",
    "        else:\n",
    "            features[\"OPTIONAL_HEADER.IMAGEBASE\"] = 0\n",
    "            features[\"OPTIONAL_HEADER.MAJOROPERATINGSYSTEM\"] = 0\n",
    "            features[\"OPTIONAL_HEADER.MAJORSUBSYSTEMVERSION\"] = 0\n",
    "            features[\"OPTIONAL_HEADER.DLLCHARACTERISTICS\"] = 0\n",
    "            features[\"OPTIONAL_HEADER.SUBSYSTEM\"] = 0\n",
    "            entropies.append(0)\n",
    "        if len(entropies) != 0:\n",
    "            features[\"PE_SECTIONS.MAXENTROPY\"] = max(entropies)\n",
    "            features[\"PE_SECTIONS.MINENTROPY\"] = min(entropies)\n",
    "            features[\"PE_SECTIONS.MEANENTROPY\"] = sum(entropies) / len(entropies)\n",
    "        else:\n",
    "            features[\"PE_SECTIONS.MAXENTROPY\"] = 0\n",
    "            features[\"PE_SECTIONS.MINENTROPY\"] = 0\n",
    "            features[\"PE_SECTIONS.MEANENTROPY\"] = 0\n",
    "       \n",
    "        entropies = []\n",
    "        if (hasattr(pe, \"DIRECTORY_ENTRY_RESOURCE\")):\n",
    "            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:\n",
    "                if resource_type.name is not None:\n",
    "                    name = str(resource_type.name)\n",
    "                else:\n",
    "                    name = str(pefile.RESOURCE_TYPE.get(resource_type.struct.Id))\n",
    "\n",
    "                if name is None:\n",
    "                    name = str(resource_type.struct.Id)\n",
    "\n",
    "                if hasattr(resource_type, 'directory'):\n",
    "                    for resource_id in resource_type.directory.entries:\n",
    "                        if hasattr(resource_id, 'directory'):\n",
    "                            for resource_lang in resource_id.directory.entries:\n",
    "                                if hasattr(resource_lang, \"data\"):\n",
    "                                    data = pe.get_data(resource_lang.data.struct.OffsetToData, resource_lang.data.struct.Size)\n",
    "                                    entropies.append(entropy(data))\n",
    "                                else:\n",
    "                                    entropies.append(0)\n",
    "        else:\n",
    "            entropies.append(0)\n",
    "        if len(entropies) != 0:\n",
    "            features[\"RESOURCES.MAXENTROPY\"] = max(entropies)\n",
    "            features[\"RESOURCES.MINENTROPY\"] = min(entropies)\n",
    "        else:\n",
    "            features[\"RESOURCES.MAXENTROPY\"] = 0\n",
    "            features[\"RESOURCES.MINENTROPY\"] = 0\n",
    "        \n",
    "        if (hasattr(pe, \"VS_VERSIONINFO\")):\n",
    "            features[\"VS_VERSIONINFO.Length\"] = pe.VS_VERSIONINFO[0].Length\n",
    "        else:\n",
    "            features[\"VS_VERSIONINFO.Length\"] = 0\n",
    "        # Adding the features to the dataframe\n",
    "        feature_df = feature_df.append(features, ignore_index=True)\n",
    "\n",
    "    except pefile.PEFormatError:\n",
    "        print(\"Error: Not a valid PE file\")\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    print('Classification Report')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('Confusion Matrix')\n",
    "    confused = confusion_matrix(y_test, y_pred)\n",
    "    f = plt.figure(figsize=(15,15))\n",
    "    ax = f.add_subplot()\n",
    "    sns.heatmap(confused, annot=True, fmt='g', ax=ax)\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels([\"Malicious\", \"Benign\"])\n",
    "    ax.yaxis.set_ticklabels([\"Malicious\", \"Benign\"])\n",
    "    plt.show()\n",
    "    print('True Negative: ' + str(confused[0][0]))\n",
    "    print('True Positive: ' + str(confused[1][1]))\n",
    "    print('False Negative: ' + str(confused[0][1]))\n",
    "    print('False Positive: ' + str(confused[1][0]))\n",
    "    return\n",
    "\n",
    "def main():\n",
    "    # MODEL OUTPUT LOCATION\n",
    "    model_file = \"model.sav\"\n",
    "    # SELECTED FEATURES OUTPUT LOCATION\n",
    "    feature_file = \"selected_features.txt\"\n",
    "    # TRAINING SAMPLE LOCATION\n",
    "    train_dir = \".\\samples\\\\training\"\n",
    "    # TESTING SAMPLE LOCATION\n",
    "    test_dir = \".\\samples\\\\validation\"\n",
    "    # CREATING THE TRAINING DATASET\n",
    "    print(\"Creating feature matrix for training data\")\n",
    "    train, selected_byte_features, selected_opcode_features_1, selected_opcode_features_2 = create_training_feature_vectors(train_dir)\n",
    "    x_train = train.loc[:, train.columns != \"CLASSIFICATION\"]\n",
    "    x_train = x_train.drop(columns=[\"SAMPLE\"])\n",
    "    y_train = train[\"CLASSIFICATION\"]\n",
    "    # CREATING AND TRAINING THE RFC CLASSIFIER\n",
    "    print(\"Training the model\")\n",
    "    model = RandomForestClassifier(n_estimators=1000).fit(x_train, y_train)\n",
    "    # CREATING THE TESTING DATASET\n",
    "    print(\"Creating feature matrix for validation data\")\n",
    "    test = create_validation_feature_vectors(test_dir, selected_byte_features, selected_opcode_features_1, selected_opcode_features_2)\n",
    "    x_test = test.loc[:, test.columns != \"CLASSIFICATION\"]\n",
    "    x_test = x_test.drop(columns=[\"SAMPLE\"])\n",
    "    y_test = test[\"CLASSIFICATION\"]\n",
    "    # EVALUATING THE RFC CLASSIFIER\n",
    "    print(\"Evaluating the model\")\n",
    "    evaluate_model(model, x_test, y_test)\n",
    "    # SAVING THE TRAINED MODEL\n",
    "    pickle.dump(model, open(model_file, 'wb'))\n",
    "    # SAVING THE SELECTED FEATURES\n",
    "    with open(feature_file, 'w') as f:\n",
    "        f.write(\"BI-GRAM BYTE FEATURES\\n\")\n",
    "        for feature in selected_byte_features:\n",
    "            f.write(str(feature) + '\\n')\n",
    "        f.write(\"BI-GRAM OPCODE FEATURES\\n\")\n",
    "        for feature in selected_opcode_features_1:\n",
    "            f.write(str(feature) + '\\n')\n",
    "        f.write(\"TRI-GRAM OPCODE FEATURES\\n\")\n",
    "        for feature in selected_opcode_features_2:\n",
    "            f.write(str(feature) + '\\n')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb84d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pefile\n",
    "from defender.classifier import create_validation_feature_vectors,create_training_feature_vectors\n",
    "\n",
    "# Load the saved model\n",
    "with open('model.sav', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "\n",
    "    train, selected_byte_features, selected_opcode_features_1, selected_opcode_features_2 = create_training_feature_vectors(\"defender/pe-machine-learning-dataset\")\n",
    "    test = create_validation_feature_vectors('defender/pe-machine-learning-dataset', selected_byte_features, selected_opcode_features_1, selected_opcode_features_2)\n",
    "    x_test = test.loc[:, test.columns != \"CLASSIFICATION\"]\n",
    "    x_test = x_test.drop(columns=[\"SAMPLE\"])\n",
    "    # Make a prediction using the loaded model\n",
    "    prediction = model.predict(x_test)\n",
    "\n",
    "    # Print the prediction\n",
    "    print(prediction)\n",
    "    print(len(prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
