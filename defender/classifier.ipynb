{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED IMPORTS\n",
    "import os                                               # Directory walking for file loading\n",
    "import math                                             # Logarithm function\n",
    "import pandas as pd                                     # Dataframe managment\n",
    "import pefile                                           # Header feature extraction\n",
    "import dis                                              # x86 disassembly\n",
    "import pickle                                           # Model saving\n",
    "import numpy as np                                      # Array managment\n",
    "from io import StringIO                                 # Reading disassembly result\n",
    "from sklearn.feature_selection import SelectFromModel   # Feature dimensionality reduction\n",
    "from sklearn.ensemble import RandomForestClassifier     # Random Forest Classifier\n",
    "from collections import Counter                         # Entropy calculations\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# EVALUATION IMPORTS\n",
    "from matplotlib import pyplot as plt                    # Output plotting\n",
    "import seaborn as sns                                   # Heatmap of confusion matrix\n",
    "from sklearn.metrics import confusion_matrix            # Confusion matrix\n",
    "from sklearn.metrics import classification_report       # Classification report\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "MAX_BYTES = 50000\n",
    "\n",
    "def entropy(data):\n",
    "    \"\"\"Calculate the entropy of a chunk of data.\"\"\"\n",
    "    if not data:\n",
    "        return 0.0\n",
    "    occurences = Counter(bytearray(data))\n",
    "    entropy = 0\n",
    "    for x in occurences.values():\n",
    "        p_x = float(x) / len(data)\n",
    "        entropy -= p_x * math.log(p_x, 2)\n",
    "    return entropy\n",
    "\n",
    "def get_header_features(pe, features):\n",
    "    # Get FILE_HEADER features\n",
    "    if (hasattr(pe, \"FILE_HEADER\")):\n",
    "        features[\"FILE_HEADER.MACHINE\"] = pe.FILE_HEADER.Machine\n",
    "        features[\"FILE_HEADER.SIZEOFOPTIONALHEADER\"] = pe.FILE_HEADER.SizeOfOptionalHeader\n",
    "        features[\"FILE_HEADER.CHARACTERISTICS\"] = pe.FILE_HEADER.Characteristics\n",
    "    else:\n",
    "        features[\"FILE_HEADER.MACHINE\"] = 0\n",
    "        features[\"FILE_HEADER.SIZEOFOPTIONALHEADER\"] = 0\n",
    "        features[\"FILE_HEADER.CHARACTERISTICS\"] = 0\n",
    "    # Get OPTIONAL_HEADER features\n",
    "    if (hasattr(pe, \"OPTIONAL_HEADER\")):\n",
    "        features[\"OPTIONAL_HEADER.IMAGEBASE\"] = pe.OPTIONAL_HEADER.ImageBase\n",
    "        features[\"OPTIONAL_HEADER.MAJOROPERATINGSYSTEM\"] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion\n",
    "        features[\"OPTIONAL_HEADER.MAJORSUBSYSTEMVERSION\"] = pe.OPTIONAL_HEADER.MajorSubsystemVersion\n",
    "        features[\"OPTIONAL_HEADER.DLLCHARACTERISTICS\"] = pe.OPTIONAL_HEADER.DllCharacteristics\n",
    "        features[\"OPTIONAL_HEADER.SUBSYSTEM\"] = pe.OPTIONAL_HEADER.Subsystem\n",
    "        entropies = []\n",
    "        for section in pe.sections:\n",
    "            entropies.append(section.get_entropy())\n",
    "        features[\"PE_SECTIONS.MAXENTROPY\"] = max(entropies)\n",
    "        features[\"PE_SECTIONS.MINENTROPY\"] = min(entropies)\n",
    "        features[\"PE_SECTIONS.MEANENTROPY\"] = sum(entropies) / len(entropies)\n",
    "    else:\n",
    "        features[\"OPTIONAL_HEADER.IMAGEBASE\"] = 0\n",
    "        features[\"OPTIONAL_HEADER.MAJOROPERATINGSYSTEM\"] = 0\n",
    "        features[\"OPTIONAL_HEADER.MAJORSUBSYSTEMVERSION\"] = 0\n",
    "        features[\"OPTIONAL_HEADER.DLLCHARACTERISTICS\"] = 0\n",
    "        features[\"OPTIONAL_HEADER.SUBSYSTEM\"] = 0\n",
    "        features[\"PE_SECTIONS.MAXENTROPY\"] = 0\n",
    "        features[\"PE_SECTIONS.MINENTROPY\"] = 0\n",
    "        features[\"PE_SECTIONS.MEANENTROPY\"] = 0\n",
    "    # Get DIRECTORY_ENTRY_RESOURCE features\n",
    "    if (hasattr(pe, \"DIRECTORY_ENTRY_RESOURCE\")):\n",
    "        # Find all resources in the PE and calculate their entropy\n",
    "        entropies = []\n",
    "        for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:\n",
    "            if resource_type.name is not None:\n",
    "                name = str(resource_type.name)\n",
    "            else:\n",
    "                name = str(pefile.RESOURCE_TYPE.get(resource_type.struct.Id))\n",
    "            if name is None:\n",
    "                name = str(resource_type.struct.Id)\n",
    "            if hasattr(resource_type, 'directory'):\n",
    "                for resource_id in resource_type.directory.entries:\n",
    "                    if hasattr(resource_id, 'directory'):\n",
    "                        for resource_lang in resource_id.directory.entries:\n",
    "                            if hasattr(resource_lang, \"data\"):\n",
    "                                try:\n",
    "                                    data = pe.get_data(resource_lang.data.struct.OffsetToData, resource_lang.data.struct.Size)\n",
    "                                    entropies.append(entropy(data))\n",
    "                                except:\n",
    "                                    entropies.append(0)\n",
    "        if len(entropies) > 0:\n",
    "            features[\"RESOURCES.MAXENTROPY\"] = max(entropies)\n",
    "            features[\"RESOURCES.MINENTROPY\"] = min(entropies)\n",
    "        else:\n",
    "            features[\"RESOURCES.MAXENTROPY\"] = 0\n",
    "            features[\"RESOURCES.MINENTROPY\"] = 0\n",
    "    else:\n",
    "        features[\"RESOURCES.MAXENTROPY\"] = 0\n",
    "        features[\"RESOURCES.MINENTROPY\"] = 0   \n",
    "    # Get VS_VERSIONINFO feature\n",
    "    if (hasattr(pe, \"VS_VERSIONINFO\")):\n",
    "        features[\"VS_VERSIONINFO.Length\"] = pe.VS_VERSIONINFO[0].Length\n",
    "    else:\n",
    "        features[\"VS_VERSIONINFO.Length\"] = 0\n",
    "    # Return the final header features\n",
    "    return features\n",
    "\n",
    "def get_byte_file(pe):\n",
    "    try:\n",
    "        global MAX_BYTES\n",
    "        if pe.OPTIONAL_HEADER.SizeOfCode > MAX_BYTES:\n",
    "            byte_file = pe.get_data(pe.OPTIONAL_HEADER.BaseOfCode, MAX_BYTES)\n",
    "        else:\n",
    "            byte_file = pe.get_data(pe.OPTIONAL_HEADER.BaseOfCode, pe.OPTIONAL_HEADER.SizeOfCode)\n",
    "        return byte_file\n",
    "    except:\n",
    "        return bytearray()\n",
    "    \n",
    "def get_training_byte_features(byte_files, classifications):\n",
    "    # Initialize the set of unique bi-gram byte features\n",
    "    all_unique_bi_grams = set()\n",
    "    each_file_bi_grams = []\n",
    "    # Iterate over the byte files\n",
    "    for sample, byte_file in byte_files.items():\n",
    "        print(\"Collecting bi-gram byte features for sample \" + str(sample))\n",
    "        # Identify new unique bi-grams in the current byte file\n",
    "        cur_unique_bi_grams = set()\n",
    "        prev_byte = None\n",
    "        for i, byte in enumerate(byte_file):\n",
    "            # Creating the bi-gram if enough history exists\n",
    "            if prev_byte != None:\n",
    "                bi_gram = hex(prev_byte) + \" \" + hex(byte)\n",
    "                # If the current bi-gram has not been seen in the current file\n",
    "                if not bi_gram in cur_unique_bi_grams:\n",
    "                    cur_unique_bi_grams.add(bi_gram)\n",
    "                # If the current bi-gram has not been seen in any file\n",
    "                if not bi_gram in all_unique_bi_grams:\n",
    "                    all_unique_bi_grams.add(bi_gram)\n",
    "            # Moving the sliding window\n",
    "            prev_byte = byte\n",
    "        # Add the set of unique bi-grams for the current file to the list\n",
    "        each_file_bi_grams.append(cur_unique_bi_grams)\n",
    "\n",
    "    # Initialize the bi-gram byte feature matrix\n",
    "    num_rows = len(byte_files)\n",
    "    num_cols = len(list(all_unique_bi_grams))\n",
    "    byte_bi_gram_features_list = [[0]*num_cols for i in range(num_rows)]\n",
    "\n",
    "    # One-hot-encoding every sample with the combination of all encountered features\n",
    "    for row, file_bi_grams in enumerate(each_file_bi_grams):\n",
    "        print(\"One-hot-encoding bi-gram byte features for sample \" + str(list(byte_files)[row]))\n",
    "        for col, bi_gram in enumerate(all_unique_bi_grams):\n",
    "            if bi_gram in file_bi_grams:\n",
    "                byte_bi_gram_features_list[row][col] = 1\n",
    "\n",
    "    # Convert byte_bi_gram_features_list to a NumPy array\n",
    "    byte_bi_gram_features_array = np.array(byte_bi_gram_features_list)\n",
    "\n",
    "    # Selecting top 200 bi-gram byte features\n",
    "    print(\"Selecting top 200 bi-gram byte features\")\n",
    "    selector = SelectKBest(chi2, k=200)\n",
    "    selector.fit(byte_bi_gram_features_array, list(classifications))\n",
    "    selections = selector.get_support()\n",
    "\n",
    "    # Copy the selected features to another matrix\n",
    "    print(\"Copying selected bi-gram byte features\")\n",
    "    selected_byte_bi_gram_features = byte_bi_gram_features_array[:, selections]\n",
    "\n",
    "    print(\"Converting selected bi-gram byte features to DataFrame\")\n",
    "    byte_bi_gram_features = pd.DataFrame(selected_byte_bi_gram_features, columns=np.array(list(all_unique_bi_grams))[selections])\n",
    "    \n",
    "    return byte_bi_gram_features\n",
    "\n",
    "def get_validation_byte_features(byte_files, selected_byte_features):\n",
    "    # Initialize the set of unique bi-gram byte features\n",
    "    each_file_bi_grams = []\n",
    "    # Iterate over the byte files\n",
    "    for sample, byte_file in byte_files.items():\n",
    "        print(\"Collecting bi-gram byte features for sample \" + str(sample))\n",
    "        # Identify new unique bi-grams in the current byte file\n",
    "        cur_unique_bi_grams = set()\n",
    "        prev_byte = None\n",
    "        for i, byte in enumerate(byte_file):\n",
    "            # Creating the bi-gram if enough history exists\n",
    "            if prev_byte != None:\n",
    "                bi_gram = hex(prev_byte) + \" \" + hex(byte)\n",
    "                # If the current bi-gram has not been seen in the current file\n",
    "                if not bi_gram in cur_unique_bi_grams:\n",
    "                    cur_unique_bi_grams.add(bi_gram)\n",
    "            # Moving the sliding window\n",
    "            prev_byte = byte\n",
    "        # Add the set of unique bi-grams for the current file to the list\n",
    "        each_file_bi_grams.append(cur_unique_bi_grams)\n",
    "    # Initialize the bi-gram byte feature matrix\n",
    "    num_rows = len(byte_files)\n",
    "    num_cols = len(selected_byte_features)\n",
    "    byte_bi_gram_features_list = [[0]*num_cols for i in range(num_rows)]             \n",
    "    # One-hot-encoding every sample with the combination of all encountered features\n",
    "    for row, file_bi_grams in enumerate(each_file_bi_grams):\n",
    "        print(\"One-hot-encoding bi-gram byte features for sample \" + str(list(byte_files)[row]))\n",
    "        for col, bi_gram in enumerate(selected_byte_features):\n",
    "            if bi_gram in file_bi_grams:\n",
    "                byte_bi_gram_features_list[row][col] = 1\n",
    "    # Convert the bi-gram byte features into a dataframe object\n",
    "    print(\"Converting bi-gram byte features to DataFrame\")\n",
    "    byte_bi_gram_features = pd.DataFrame(byte_bi_gram_features_list, columns=selected_byte_features)\n",
    "    return byte_bi_gram_features\n",
    "\n",
    "def get_classification_byte_features(byte_file, selected_byte_features):\n",
    "    # Identify new unique bi-grams in the byte file\n",
    "    unique_bi_grams = set()\n",
    "    prev_byte = None\n",
    "    for i, byte in enumerate(byte_file):\n",
    "        # Creating the bi-gram if enough history exists\n",
    "        if prev_byte != None:\n",
    "            bi_gram = hex(prev_byte) + \" \" + hex(byte)\n",
    "            # If the current bi-gram has not been seen in the current file\n",
    "            if not bi_gram in unique_bi_grams:\n",
    "                unique_bi_grams.add(bi_gram)\n",
    "        # Moving the sliding window\n",
    "        prev_byte = byte\n",
    "    # Initialize the bi-gram byte feature matrix\n",
    "    byte_bi_gram_features = dict.fromkeys(selected_byte_features, 0)          \n",
    "    # One-hot-encoding the sample with the combination of all encountered features\n",
    "    for bi_gram in selected_byte_features:\n",
    "        if bi_gram in list(unique_bi_grams):\n",
    "            byte_bi_gram_features[bi_gram] = 1\n",
    "    return byte_bi_gram_features\n",
    "    \n",
    "def get_asm_file(pe):\n",
    "    # Try reading the instructions as bytes\n",
    "    try:\n",
    "        global MAX_BYTES\n",
    "        if pe.OPTIONAL_HEADER.SizeOfCode > MAX_BYTES:\n",
    "            file_bytes = pe.get_data(pe.OPTIONAL_HEADER.BaseOfCode, MAX_BYTES)\n",
    "        else:\n",
    "            file_bytes = pe.get_data(pe.OPTIONAL_HEADER.BaseOfCode, pe.OPTIONAL_HEADER.SizeOfCode)\n",
    "    except:\n",
    "        # If fails return list of no instructions found\n",
    "        return []\n",
    "    # Do weird try/catch since dis.dis is broken\n",
    "    out = StringIO()\n",
    "    try:\n",
    "        dis.dis(x=file_bytes, file=out)\n",
    "    except:\n",
    "        pass\n",
    "    # Parse the disasembled instructions and create a list of OPCODES\n",
    "    lines = out.getvalue().split('\\n')\n",
    "    lines = lines[0:len(lines)-1]\n",
    "    asm_file = [line.strip().strip('>>').strip().split(' ')[1] for line in lines if line.strip().strip('>>').strip()]\n",
    "    return asm_file\n",
    "    \n",
    "\n",
    "def get_training_asm_features(asm_files, classifications):\n",
    "    # Initialize the set of unique bi-gram and tri-gram OPCODE features\n",
    "    all_unique_bi_grams = set()\n",
    "    all_unique_tri_grams = set()\n",
    "    each_file_bi_grams = []\n",
    "    each_file_tri_grams = []\n",
    "    # Iterate over the ASM files\n",
    "    for sample, asm_file in asm_files.items():\n",
    "        print(\"Collecting bi-gram and tri-gram OPCODE features for sample \" + str(sample))\n",
    "        # Identify new unique bi-grams and tri-grams in the current ASM file\n",
    "        cur_unique_bi_grams = set()\n",
    "        cur_unique_tri_grams = set()\n",
    "        prev_opcode1 = None\n",
    "        prev_opcode2 = None\n",
    "        for i, opcode in enumerate(asm_file):\n",
    "            # Creating the bi-gram if enough history exists\n",
    "            if prev_opcode1 != None:\n",
    "                bi_gram = prev_opcode1 + \" \" + opcode\n",
    "                # If the current bi-gram has not been seen in the current file\n",
    "                if not bi_gram in cur_unique_bi_grams:\n",
    "                    cur_unique_bi_grams.add(bi_gram)\n",
    "                # If the current bi-gram has not been seen in any file\n",
    "                if not bi_gram in all_unique_bi_grams:\n",
    "                    all_unique_bi_grams.add(bi_gram)\n",
    "            # Creating the tri-gram if enough history exists\n",
    "            if prev_opcode2 != None:\n",
    "                tri_gram = prev_opcode2 + \" \" + prev_opcode1 + \" \" + opcode\n",
    "                # If the current tri-gram has not been seen in the current file\n",
    "                if not tri_gram in cur_unique_tri_grams:\n",
    "                    cur_unique_tri_grams.add(tri_gram)\n",
    "                # If the current tri-gram has not been seen in any file\n",
    "                if not tri_gram in all_unique_tri_grams:\n",
    "                    all_unique_tri_grams.add(tri_gram)\n",
    "            # Moving the sliding window\n",
    "            prev_opcode2 = prev_opcode1\n",
    "            prev_opcode1 = opcode\n",
    "        # Add the set of unique bi-grams and tri-grams for the current file to the list\n",
    "        each_file_bi_grams.append(cur_unique_bi_grams)\n",
    "        each_file_tri_grams.append(cur_unique_tri_grams)\n",
    "    # Initialize the bi-gram OPCODE feature matrix\n",
    "    num_rows = len(asm_files)\n",
    "    num_cols1 = len(list(all_unique_bi_grams))\n",
    "    num_cols2 = len(list(all_unique_tri_grams))\n",
    "    opcode_bi_gram_features_list = [[0]*num_cols1 for i in range(num_rows)]\n",
    "    opcode_tri_gram_features_list = [[0]*num_cols2 for i in range(num_rows)]\n",
    "\n",
    "    opcode_bi_gram_features_array = np.array(opcode_bi_gram_features_list)\n",
    "    opcode_tri_gram_features_array = np.array(opcode_tri_gram_features_list)\n",
    "\n",
    "    # One-hot-encoding every sample with the combination of all encountered features\n",
    "    for row, file_bi_grams in enumerate(each_file_bi_grams):\n",
    "        print(\"One-hot-encoding bi-gram OPCODE features for sample \" + str(list(asm_files)[row]))\n",
    "        for col, bi_gram in enumerate(all_unique_bi_grams):\n",
    "            if bi_gram in file_bi_grams:\n",
    "                opcode_bi_gram_features_array[row][col] = 1\n",
    "    for row, file_tri_grams in enumerate(each_file_tri_grams):\n",
    "        print(\"One-hot-encoding tri-gram OPCODE features for sample \" + str(list(asm_files)[row]))\n",
    "        for col, tri_gram in enumerate(all_unique_tri_grams):\n",
    "            if tri_gram in file_tri_grams:\n",
    "                opcode_tri_gram_features_array[row][col] = 1\n",
    "    # Creating the feature selector model for top 100 features\n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(n_estimators=1000), max_features=100)\n",
    "    # Selecting top 100 bi-gram opcode features\n",
    "    print(\"Selecting top 100 bi-gram OPCODE features\")\n",
    "    selector.fit(opcode_bi_gram_features_array, list(classifications))\n",
    "    selections = selector.get_support()\n",
    "    # Copying the selected bi-gram features to another matrix\n",
    "    print(\"Copying selected bi-gram OPCODE features\")\n",
    "    selected_opcode_bi_gram_features_dict = {}\n",
    "    selected_indicies = [i for i, e in enumerate(selections) if e == True]\n",
    "    for index in selected_indicies:\n",
    "        selected_opcode_bi_gram_features_dict[list(all_unique_bi_grams)[index]] = [row[index] for row in opcode_bi_gram_features_array]\n",
    "    print(\"Converting selected bi-gram byte features to DataFrame\")\n",
    "    opcode_bi_gram_features = pd.DataFrame(selected_opcode_bi_gram_features_dict)\n",
    "    # Selecting top 100 tri-gram opcode features\n",
    "    print(\"Selecting top 100 tri-gram OPCODE features\")\n",
    "    selector.fit(opcode_tri_gram_features_array, list(classifications))\n",
    "    selections = selector.get_support()\n",
    "    # Copy the selected features to another matrix\n",
    "    print(\"Copying selected tri-gram OPCODE features\")\n",
    "    selected_opcode_tri_gram_features_dict = {}\n",
    "    selected_indicies = [i for i, e in enumerate(selections) if e == True]\n",
    "    for index in selected_indicies:\n",
    "        selected_opcode_tri_gram_features_dict[list(all_unique_tri_grams)[index]] = [row[index] for row in opcode_tri_gram_features_array]\n",
    "    print(\"Converting selected bi-gram byte features to DataFrame\")\n",
    "    opcode_tri_gram_features = pd.DataFrame(selected_opcode_tri_gram_features_dict)\n",
    "    return opcode_bi_gram_features, opcode_tri_gram_features\n",
    "\n",
    "def get_validation_asm_features(asm_files, selected_opcode_features_1, selected_opcode_features_2):\n",
    "    # Initialize the set of unique bi-gram and tri-gram OPCODE features\n",
    "    each_file_bi_grams = []\n",
    "    each_file_tri_grams = []\n",
    "    # Iterate over the ASM files\n",
    "    for sample, asm_file in asm_files.items():\n",
    "        print(\"Collecting bi-gram and tri-gram OPCODE features for sample \" + str(sample))\n",
    "        # Identify new unique bi-grams and tri-grams in the current ASM file\n",
    "        cur_unique_bi_grams = set()\n",
    "        cur_unique_tri_grams = set()\n",
    "        prev_opcode1 = None\n",
    "        prev_opcode2 = None\n",
    "        for i, opcode in enumerate(asm_file):\n",
    "            # Creating the bi-gram if enough history exists\n",
    "            if prev_opcode1 != None:\n",
    "                bi_gram = prev_opcode1 + \" \" + opcode\n",
    "                # If the current bi-gram has not been seen in the current file\n",
    "                if not bi_gram in cur_unique_bi_grams:\n",
    "                    cur_unique_bi_grams.add(bi_gram)\n",
    "            # Creating the tri-gram if enough history exists\n",
    "            if prev_opcode2 != None:\n",
    "                tri_gram = prev_opcode2 + \" \" + prev_opcode1 + \" \" + opcode\n",
    "                # If the current tri-gram has not been seen in the current file\n",
    "                if not tri_gram in cur_unique_tri_grams:\n",
    "                    cur_unique_tri_grams.add(tri_gram)\n",
    "            # Moving the sliding window\n",
    "            prev_opcode2 = prev_opcode1\n",
    "            prev_opcode1 = opcode\n",
    "        # Add the set of unique bi-grams and tri-grams for the current file to the list\n",
    "        each_file_bi_grams.append(cur_unique_bi_grams)\n",
    "        each_file_tri_grams.append(cur_unique_tri_grams)\n",
    "    # Initialize the bi-gram OPCODE feature matrix\n",
    "    num_rows = len(asm_files)\n",
    "    num_cols1 = len(selected_opcode_features_1)\n",
    "    num_cols2 = len(selected_opcode_features_2)\n",
    "    opcode_bi_gram_features_list = [[0]*num_cols1 for i in range(num_rows)]\n",
    "    opcode_tri_gram_features_list = [[0]*num_cols2 for i in range(num_rows)]              \n",
    "    # One-hot-encoding every sample with the combination of all encountered features\n",
    "    for row, file_bi_grams in enumerate(each_file_bi_grams):\n",
    "        print(\"One-hot-encoding bi-gram OPCODE features for sample \" + str(list(asm_files)[row]))\n",
    "        for col, bi_gram in enumerate(selected_opcode_features_1):\n",
    "            if bi_gram in file_bi_grams:\n",
    "                opcode_bi_gram_features_list[row][col] = 1\n",
    "    for row, file_tri_grams in enumerate(each_file_tri_grams):\n",
    "        print(\"One-hot-encoding tri-gram OPCODE features for sample \" + str(list(asm_files)[row]))\n",
    "        for col, tri_gram in enumerate(selected_opcode_features_2):\n",
    "            if tri_gram in file_tri_grams:\n",
    "                opcode_tri_gram_features_list[row][col] = 1\n",
    "    # Convert the bi-gram OPCODE features into a dataframe object\n",
    "    print(\"Converting bi-gram OPCODE features to DataFrame\")\n",
    "    opcode_bi_gram_features = pd.DataFrame(opcode_bi_gram_features_list, columns=selected_opcode_features_1)\n",
    "    # Convert the tri-gram OPCODE features into a dataframe object\n",
    "    print(\"Converting tri-gram OPCODE features to DataFrame\")\n",
    "    opcode_tri_gram_features = pd.DataFrame(opcode_tri_gram_features_list, columns=selected_opcode_features_2)\n",
    "    return opcode_bi_gram_features, opcode_tri_gram_features\n",
    "\n",
    "def get_classification_asm_features(asm_file, selected_opcode_features_1, selected_opcode_features_2):\n",
    "    # Identify unique bi-grams and tri-grams\n",
    "    unique_bi_grams = set()\n",
    "    unique_tri_grams = set()\n",
    "    prev_opcode1 = None\n",
    "    prev_opcode2 = None\n",
    "    for i, opcode in enumerate(asm_file):\n",
    "        # Creating the bi-gram if enough history exists\n",
    "        if prev_opcode1 != None:\n",
    "            bi_gram = prev_opcode1 + \" \" + opcode\n",
    "            # If the current bi-gram has not been seen\n",
    "            if not bi_gram in unique_bi_grams:\n",
    "                unique_bi_grams.add(bi_gram)\n",
    "        # Creating the tri-gram if enough history exists\n",
    "        if prev_opcode2 != None:\n",
    "            tri_gram = prev_opcode2 + \" \" + prev_opcode1 + \" \" + opcode\n",
    "            # If the current tri-gram has not been seen\n",
    "            if not tri_gram in unique_tri_grams:\n",
    "                unique_tri_grams.add(tri_gram)\n",
    "        # Moving the sliding window\n",
    "        prev_opcode2 = prev_opcode1\n",
    "        prev_opcode1 = opcode\n",
    "\n",
    "    # Initialize the bi-gram OPCODE feature matrix\n",
    "    opcode_bi_gram_features = dict.fromkeys(selected_opcode_features_1, 0)          \n",
    "    # One-hot-encoding the sample with the combination of all encountered features\n",
    "    for bi_gram in selected_opcode_features_1:\n",
    "        if bi_gram in list(unique_bi_grams):\n",
    "            opcode_bi_gram_features[bi_gram] = 1\n",
    "    # Initialize the tri-gram OPCODE feature matrix\n",
    "    opcode_tri_gram_features = dict.fromkeys(selected_opcode_features_2, 0)          \n",
    "    # One-hot-encoding the sample with the combination of all encountered features\n",
    "    for tri_gram in selected_opcode_features_2:\n",
    "        if tri_gram in list(unique_tri_grams):\n",
    "            opcode_tri_gram_features[tri_gram] = 1\n",
    "    return opcode_bi_gram_features, opcode_tri_gram_features\n",
    "\n",
    "def bpca(data, n_components=64, max_iterations=100, learning_rate=1e-3):\n",
    "    data = np.where(data > 0, 1, 0)\n",
    "    data_mean = np.mean(data, axis=0)\n",
    "    data_centered = data - data_mean\n",
    "    num_samples = data.shape[0]\n",
    "\n",
    "    W = np.random.randn(n_components, data.shape[1])\n",
    "    W_old = np.zeros_like(W)\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        Y = np.where(np.dot(W, data_centered.T) > 0, 1, -1)\n",
    "        gradient = 1 / num_samples * np.dot(Y, data_centered)\n",
    "        W = W + learning_rate * gradient\n",
    "\n",
    "        if np.linalg.norm(W - W_old) < 1e-6:\n",
    "            break\n",
    "        W_old = W.copy()\n",
    "\n",
    "    return W\n",
    "\n",
    "def get_binary_pca_features(byte_files):\n",
    "    features = []\n",
    "    index = []\n",
    "    for file_path, file_data in byte_files.items():\n",
    "        # Convert binary data to numerical data\n",
    "        numerical_data = np.frombuffer(file_data, dtype=np.uint8)\n",
    "\n",
    "        # Reshape numerical_data into a 2D array with a single row\n",
    "        numerical_data = numerical_data.reshape(1, -1)\n",
    "        bpca_weights = bpca(numerical_data)\n",
    "\n",
    "        transformed_data = np.dot(bpca_weights, (numerical_data - np.mean(numerical_data, axis=0)).T).T\n",
    "        feature_vector = np.mean(transformed_data, axis=0)\n",
    "\n",
    "        features.append(feature_vector)\n",
    "        index.append(file_path)\n",
    "\n",
    "    feature_matrix = np.array(features)\n",
    "    df = pd.DataFrame(feature_matrix, index=index)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_training_feature_vectors(sample_dir):\n",
    "    # Creating initial header feature dataframe\n",
    "    header_feature_df = pd.DataFrame()\n",
    "    # Creating structure to store all byte and ASM files\n",
    "    byte_files = {}\n",
    "    asm_files = {}\n",
    "    # Iterating through all samples in the samples directory\n",
    "    for root, dirs, files in os.walk(sample_dir):\n",
    "        print(\"rootsdfs\", root)\n",
    "        for file in files:\n",
    "            sample = os.path.join(root,file)\n",
    "            print(\"Processing sample \" + str(sample))\n",
    "            # Creating initial entry for the current sample\n",
    "            header_features = {}\n",
    "            header_features[\"SAMPLE\"] = (sample)\n",
    "            if (\"malicious\" in root):\n",
    "                header_features[\"CLASSIFICATION\"] = 1 \n",
    "            else:\n",
    "                header_features[\"CLASSIFICATION\"] = 0\n",
    "            # Try to process the sample as a PE file\n",
    "            try:\n",
    "                pe = pefile.PE(sample)\n",
    "            except:\n",
    "                # Sample not a PE file so put empty features and add it to the feature dataframe\n",
    "                print(\"Sample \" + str(sample) + \" is not a PE file, creating empty feature vector\")\n",
    "                header_feature_df = header_feature_df.append(header_features, ignore_index=True)\n",
    "                byte_files[sample] = bytearray()\n",
    "                asm_files[sample] = []\n",
    "                continue\n",
    "            # Collecting PE header features from the current sample\n",
    "            print(\"Collecting header features for sample \" + str(sample))\n",
    "            header_features = get_header_features(pe, header_features)\n",
    "            header_feature_df = header_feature_df.append(header_features, ignore_index=True)\n",
    "            # Gathering byte file for the current sample\n",
    "            print(\"Gathering byte file for sample \" + str(sample))\n",
    "            byte_files[sample] = get_byte_file(pe)\n",
    "            # Gathering ASM file for the current sample\n",
    "            print(\"Gathering ASM file for sample \" + str(sample))\n",
    "            asm_files[sample] = get_asm_file(pe)\n",
    "            # Creating binary PCA features\n",
    "            print(\"Creating binary PCA features\")\n",
    "    binary_pca_features = get_binary_pca_features(byte_files)\n",
    "    # Creating byte-based feature matrix\n",
    "    print(\"Creating byte-based feature matrix\")\n",
    "    byte_bi_gram_features = get_training_byte_features(byte_files, list(header_feature_df[\"CLASSIFICATION\"]))\n",
    "    # Creating opcode-based feature matrix\n",
    "    print(\"Creating opcode-based feature matrix\")\n",
    "    opcode_bi_gram_features, opcode_tri_gram_features = get_training_asm_features(asm_files, list(header_feature_df[\"CLASSIFICATION\"]))\n",
    "    # Creating final dataset with full feature matrix\n",
    "    dataframes_to_concatenate = [header_feature_df, byte_bi_gram_features, opcode_bi_gram_features, opcode_tri_gram_features, binary_pca_features]\n",
    "    non_empty_dataframes = [df for df in dataframes_to_concatenate if not df.empty]\n",
    "    final_feature_df = pd.concat(non_empty_dataframes)\n",
    "\n",
    "    # Fill empty spaces in the dataframe with 0s\n",
    "    final_feature_df = final_feature_df.fillna(0)\n",
    "    return final_feature_df, byte_bi_gram_features.columns, opcode_bi_gram_features.columns, opcode_tri_gram_features.columns, binary_pca_features.columns\n",
    "\n",
    "def create_validation_feature_vectors(sample_dir, selected_byte_features, selected_opcode_features_1, selected_opcode_features_2):\n",
    "    # Creating initial header feature dataframe\n",
    "    header_feature_df = pd.DataFrame()\n",
    "    # Creating structure to store all byte and ASM files\n",
    "    byte_files = {}\n",
    "    asm_files = {}\n",
    "    # Iterating through all samples in the samples directory\n",
    "    for root, dirs, files in os.walk(sample_dir):\n",
    "        for file in files:\n",
    "            sample = os.path.join(root,file)\n",
    "            print(\"Processing sample \" + str(sample))\n",
    "            # Creating initial entry for the current sample\n",
    "            header_features = {}\n",
    "            header_features[\"SAMPLE\"] = (sample)\n",
    "            if (\"malicious\" in root):\n",
    "                header_features[\"CLASSIFICATION\"] = 1 \n",
    "            else:\n",
    "                header_features[\"CLASSIFICATION\"] = 0\n",
    "            # Try to process the sample as a PE file\n",
    "            try:\n",
    "                pe = pefile.PE(sample)\n",
    "            except:\n",
    "                # Sample not a PE file so put empty features and add it to the feature dataframe\n",
    "                print(\"Sample \" + str(sample) + \" is not a PE file, creating empty feature vector\")\n",
    "                header_feature_df = header_feature_df.append(header_features, ignore_index=True)\n",
    "                byte_files[sample] = bytearray()\n",
    "                asm_files[sample] = []\n",
    "                continue\n",
    "            # Collecting PE header features from the current sample\n",
    "            print(\"Collecting header features for sample \" + str(sample))\n",
    "            header_features = get_header_features(pe, header_features)\n",
    "            header_feature_df = header_feature_df.append(header_features, ignore_index=True)\n",
    "            # Gathering byte file for the current sample\n",
    "            print(\"Gathering byte file for sample \" + str(sample))\n",
    "            byte_files[sample] = get_byte_file(pe)\n",
    "            # Gathering ASM file for the current sample\n",
    "            print(\"Gathering ASM file for sample \" + str(sample))\n",
    "            asm_files[sample] = get_asm_file(pe)\n",
    "    # Creating byte-based feature matrix\n",
    "    print(\"Creating byte-based feature matrix\")\n",
    "    byte_bi_gram_features = get_validation_byte_features(byte_files, selected_byte_features)\n",
    "    # Creating opcode-based feature matrix\n",
    "    print(\"Creating opcode-based feature matrix\")\n",
    "    opcode_bi_gram_features, opcode_tri_gram_features = get_validation_asm_features(asm_files, selected_opcode_features_1, selected_opcode_features_2)\n",
    "    # Creating final dataset with full feature matrix\n",
    "    final_feature_df = pd.concat([header_feature_df, byte_bi_gram_features, opcode_bi_gram_features, opcode_tri_gram_features])\n",
    "    # Fill empty spaces in the dataframe with 0s\n",
    "    final_feature_df = final_feature_df.fillna(0)\n",
    "    return final_feature_df\n",
    "\n",
    "def create_classification_feature_vector(sample, selected_feature_path):\n",
    "    selected_byte_features, selected_opcode_features_1, selected_opcode_features_2=parse_selected_features(selected_feature_path)\n",
    "    # Collecting PE header features from the sample\n",
    "    header_features = {}\n",
    "    try:\n",
    "        pe = pefile.PE(sample)\n",
    "        # Collecting PE header features from the current sample\n",
    "        header_features = get_header_features(pe, header_features)\n",
    "        # Gathering byte file for the sample\n",
    "        byte_file = get_byte_file(pe)\n",
    "        # Gathering ASM file for the sample\n",
    "        asm_file = get_asm_file(pe)\n",
    "    except:\n",
    "        # Sample not a PE file so put empty features\n",
    "        byte_file = bytearray()\n",
    "        asm_file = []\n",
    "    # Creating byte-based features\n",
    "    byte_bi_gram_features = get_classification_byte_features(byte_file, selected_byte_features)\n",
    "    # Creating opcode-based features\n",
    "    opcode_bi_gram_features, opcode_tri_gram_features = get_classification_asm_features(asm_file, selected_opcode_features_1, selected_opcode_features_2)\n",
    "    # Creating final feature vector\n",
    "    features = {**header_features, **byte_bi_gram_features}\n",
    "    features = {**features, **opcode_bi_gram_features}\n",
    "    features = {**features, **opcode_tri_gram_features}\n",
    "    return features\n",
    "\n",
    "def parse_selected_features(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    bi_gram_byte_features = []\n",
    "    bi_gram_opcode_features = []\n",
    "    tri_gram_opcode_features = []\n",
    "\n",
    "    current_section = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if line == \"BI-GRAM BYTE FEATURES\":\n",
    "            current_section = \"bi_gram_byte\"\n",
    "        elif line == \"BI-GRAM OPCODE FEATURES\":\n",
    "            current_section = \"bi_gram_opcode\"\n",
    "        elif line == \"TRI-GRAM OPCODE FEATURES\":\n",
    "            current_section = \"tri_gram_opcode\"\n",
    "        else:\n",
    "            if current_section == \"bi_gram_byte\":\n",
    "                bi_gram_byte_features.append(line)\n",
    "            elif current_section == \"bi_gram_opcode\":\n",
    "                bi_gram_opcode_features.append(line)\n",
    "            elif current_section == \"tri_gram_opcode\":\n",
    "                tri_gram_opcode_features.append(line)\n",
    "    print(len(bi_gram_byte_features), len(bi_gram_opcode_features), len(tri_gram_opcode_features))\n",
    "    return bi_gram_byte_features, bi_gram_opcode_features, tri_gram_opcode_features\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    print('Classification Report')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('Confusion Matrix')\n",
    "    confused = confusion_matrix(y_test, y_pred)\n",
    "    f = plt.figure(figsize=(15,15))\n",
    "    ax = f.add_subplot()\n",
    "    sns.heatmap(confused, annot=True, fmt='g', ax=ax)\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels([\"Malicious\", \"Benign\"])\n",
    "    ax.yaxis.set_ticklabels([\"Malicious\", \"Benign\"])\n",
    "    plt.show()\n",
    "    print('True Negative: ' + str(confused[0][0]))\n",
    "    print('True Positive: ' + str(confused[1][1]))\n",
    "    print('False Negative: ' + str(confused[0][1]))\n",
    "    print('False Positive: ' + str(confused[1][0]))\n",
    "    return\n",
    "\n",
    "def main():\n",
    "    # MODEL OUTPUT LOCATION\n",
    "    model_file = \"model.sav\"\n",
    "    # SELECTED FEATURES OUTPUT LOCATION\n",
    "    feature_file = \"selected_features.txt\"\n",
    "    # TRAINING SAMPLE LOCATION\n",
    "    train_dir = \".\\samples\\\\training\"\n",
    "    # TESTING SAMPLE LOCATION\n",
    "    test_dir = \".\\samples\\\\validation\"\n",
    "    # CREATING THE TRAINING DATASET\n",
    "    print(\"Creating feature matrix for training data\")\n",
    "    train, selected_byte_features, selected_opcode_features_1, selected_opcode_features_2, binary_pca_features = create_training_feature_vectors(train_dir)\n",
    "    x_train = train.loc[:, train.columns != \"CLASSIFICATION\"]\n",
    "    x_train = x_train.drop(columns=[\"SAMPLE\"])\n",
    "    y_train = train[\"CLASSIFICATION\"]\n",
    "    # CREATING AND TRAINING THE RFC CLASSIFIER\n",
    "    print(\"Training the model\")\n",
    "    model = RandomForestClassifier(n_estimators=1000).fit(x_train, y_train)\n",
    "    # CREATING THE TESTING DATASET\n",
    "    print(\"Creating feature matrix for validation data\")\n",
    "    test = create_validation_feature_vectors(test_dir, selected_byte_features, selected_opcode_features_1, selected_opcode_features_2)\n",
    "    x_test = test.loc[:, test.columns != \"CLASSIFICATION\"]\n",
    "    x_test = x_test.drop(columns=[\"SAMPLE\"])\n",
    "    y_test = test[\"CLASSIFICATION\"]\n",
    "    # EVALUATING THE RFC CLASSIFIER\n",
    "    print(\"Evaluating the model\")\n",
    "    evaluate_model(model, x_test, y_test)\n",
    "    # SAVING THE TRAINED MODEL\n",
    "    pickle.dump(model, open(model_file, 'wb'))\n",
    "    # SAVING THE SELECTED FEATURES\n",
    "    with open(feature_file, 'w') as f:\n",
    "        f.write(\"BI-GRAM BYTE FEATURES\\n\")\n",
    "        for feature in selected_byte_features:\n",
    "            f.write(str(feature) + '\\n')\n",
    "        f.write(\"BI-GRAM OPCODE FEATURES\\n\")\n",
    "        for feature in selected_opcode_features_1:\n",
    "            f.write(str(feature) + '\\n')\n",
    "        f.write(\"TRI-GRAM OPCODE FEATURES\\n\")\n",
    "        for feature in selected_opcode_features_2:\n",
    "            f.write(str(feature) + '\\n')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b45fda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature matrix for training data\n",
      "rootsdfs .\\samples\\training\n",
      "rootsdfs .\\samples\\training\\goodware\n",
      "Processing sample .\\samples\\training\\goodware\\0000\n",
      "Sample .\\samples\\training\\goodware\\0000 is not a PE file, creating empty feature vector\n",
      "rootsdfs .\\samples\\training\\malicious\n",
      "Processing sample .\\samples\\training\\malicious\\0000\n",
      "Collecting header features for sample .\\samples\\training\\malicious\\0000\n",
      "Gathering byte file for sample .\\samples\\training\\malicious\\0000\n",
      "Gathering ASM file for sample .\\samples\\training\\malicious\\0000\n",
      "Creating binary PCA features\n",
      "Processing sample .\\samples\\training\\malicious\\0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukun\\AppData\\Local\\Temp\\ipykernel_38336\\3607354885.py:499: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  header_feature_df = header_feature_df.append(header_features, ignore_index=True)\n",
      "C:\\Users\\yukun\\AppData\\Local\\Temp\\ipykernel_38336\\3607354885.py:506: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  header_feature_df = header_feature_df.append(header_features, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting header features for sample .\\samples\\training\\malicious\\0021\n",
      "Gathering byte file for sample .\\samples\\training\\malicious\\0021\n",
      "Gathering ASM file for sample .\\samples\\training\\malicious\\0021\n",
      "Creating binary PCA features\n",
      "Processing sample .\\samples\\training\\malicious\\0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukun\\AppData\\Local\\Temp\\ipykernel_38336\\3607354885.py:506: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  header_feature_df = header_feature_df.append(header_features, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting header features for sample .\\samples\\training\\malicious\\0022\n",
      "Gathering byte file for sample .\\samples\\training\\malicious\\0022\n",
      "Gathering ASM file for sample .\\samples\\training\\malicious\\0022\n",
      "Creating binary PCA features\n",
      "Processing sample .\\samples\\training\\malicious\\0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukun\\AppData\\Local\\Temp\\ipykernel_38336\\3607354885.py:506: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  header_feature_df = header_feature_df.append(header_features, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting header features for sample .\\samples\\training\\malicious\\0023\n",
      "Gathering byte file for sample .\\samples\\training\\malicious\\0023\n",
      "Gathering ASM file for sample .\\samples\\training\\malicious\\0023\n",
      "Creating binary PCA features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukun\\AppData\\Local\\Temp\\ipykernel_38336\\3607354885.py:506: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  header_feature_df = header_feature_df.append(header_features, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating byte-based feature matrix\n",
      "Collecting bi-gram byte features for sample .\\samples\\training\\goodware\\0000\n",
      "Collecting bi-gram byte features for sample .\\samples\\training\\malicious\\0000\n",
      "Collecting bi-gram byte features for sample .\\samples\\training\\malicious\\0021\n",
      "Collecting bi-gram byte features for sample .\\samples\\training\\malicious\\0022\n",
      "Collecting bi-gram byte features for sample .\\samples\\training\\malicious\\0023\n",
      "One-hot-encoding bi-gram byte features for sample .\\samples\\training\\goodware\\0000\n",
      "One-hot-encoding bi-gram byte features for sample .\\samples\\training\\malicious\\0000\n",
      "One-hot-encoding bi-gram byte features for sample .\\samples\\training\\malicious\\0021\n",
      "One-hot-encoding bi-gram byte features for sample .\\samples\\training\\malicious\\0022\n",
      "One-hot-encoding bi-gram byte features for sample .\\samples\\training\\malicious\\0023\n",
      "Selecting top 200 bi-gram byte features\n",
      "Copying selected bi-gram byte features\n",
      "Converting selected bi-gram byte features to DataFrame\n",
      "Creating opcode-based feature matrix\n",
      "Collecting bi-gram and tri-gram OPCODE features for sample .\\samples\\training\\goodware\\0000\n",
      "Collecting bi-gram and tri-gram OPCODE features for sample .\\samples\\training\\malicious\\0000\n",
      "Collecting bi-gram and tri-gram OPCODE features for sample .\\samples\\training\\malicious\\0021\n",
      "Collecting bi-gram and tri-gram OPCODE features for sample .\\samples\\training\\malicious\\0022\n",
      "Collecting bi-gram and tri-gram OPCODE features for sample .\\samples\\training\\malicious\\0023\n",
      "One-hot-encoding bi-gram OPCODE features for sample .\\samples\\training\\goodware\\0000\n",
      "One-hot-encoding bi-gram OPCODE features for sample .\\samples\\training\\malicious\\0000\n",
      "One-hot-encoding bi-gram OPCODE features for sample .\\samples\\training\\malicious\\0021\n",
      "One-hot-encoding bi-gram OPCODE features for sample .\\samples\\training\\malicious\\0022\n",
      "One-hot-encoding bi-gram OPCODE features for sample .\\samples\\training\\malicious\\0023\n",
      "One-hot-encoding tri-gram OPCODE features for sample .\\samples\\training\\goodware\\0000\n",
      "One-hot-encoding tri-gram OPCODE features for sample .\\samples\\training\\malicious\\0000\n",
      "One-hot-encoding tri-gram OPCODE features for sample .\\samples\\training\\malicious\\0021\n",
      "One-hot-encoding tri-gram OPCODE features for sample .\\samples\\training\\malicious\\0022\n",
      "One-hot-encoding tri-gram OPCODE features for sample .\\samples\\training\\malicious\\0023\n",
      "Selecting top 100 bi-gram OPCODE features\n",
      "Copying selected bi-gram OPCODE features\n",
      "Converting selected bi-gram byte features to DataFrame\n",
      "Selecting top 100 tri-gram OPCODE features\n",
      "Copying selected tri-gram OPCODE features\n",
      "Converting selected bi-gram byte features to DataFrame\n"
     ]
    }
   ],
   "source": [
    "model_file = \"model.sav\"\n",
    "    # SELECTED FEATURES OUTPUT LOCATION\n",
    "feature_file = \"selected_features.txt\"\n",
    "# TRAINING SAMPLE LOCATION\n",
    "train_dir = \".\\samples\\\\training\"\n",
    "# TESTING SAMPLE LOCATION\n",
    "test_dir = \".\\samples\\\\validation\"\n",
    "# CREATING THE TRAINING DATASET\n",
    "print(\"Creating feature matrix for training data\")\n",
    "train, selected_byte_features, selected_opcode_features_1, selected_opcode_features_2, get_binary_pca_features = create_training_feature_vectors(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320ba53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
