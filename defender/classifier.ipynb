{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c9e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED IMPORTS\n",
    "import os                                               # Directory walking for file loading\n",
    "import math                                             # Logarithm function\n",
    "import pandas as pd                                     # Dataframe managment\n",
    "import pefile                                           # Header feature extraction\n",
    "import pickle                                           # Model saving\n",
    "from sklearn.feature_selection import SelectFromModel   # Feature dimensionality reduction\n",
    "from sklearn.ensemble import RandomForestClassifier     # Random Forest Classifier\n",
    "from collections import Counter                         # Entropy calculations\n",
    "\n",
    "# EVALUATION IMPORTS\n",
    "import matplotlib as plt                                # Output plotting\n",
    "import seaborn as sns                                   # Heatmap of confusion matrix\n",
    "from sklearn.metrics import confusion_matrix            # Confusion matrix\n",
    "from sklearn.metrics import classification_report       # Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760a1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL OUTPUT LOCATION\n",
    "model_file = \"model.sav\"\n",
    "\n",
    "# TRAINING SAMPLE LOCATION\n",
    "train_dir = \".\\samples\\\\training\"\n",
    "\n",
    "# TESTING SAMPLE LOCATION\n",
    "test_dir = \".\\samples\\\\validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "341d20af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(self, data):\n",
    "    \"\"\"Calculate the entropy of a chunk of data.\"\"\"\n",
    "\n",
    "    if not data:\n",
    "        return 0.0\n",
    "\n",
    "    occurences = Counter(bytearray(data))\n",
    "\n",
    "    entropy = 0\n",
    "    for x in occurences.values():\n",
    "        p_x = float(x) / len(data)\n",
    "        entropy -= p_x * math.log(p_x, 2)\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ce93d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vectors(sample_dir):\n",
    "    # Creating initial feature dataframe\n",
    "    feature_df = pd.DataFrame()\n",
    "\n",
    "    # Creating initial byte bi-gram dataframe\n",
    "    byte_bi_gram_features = pd.DataFrame(columns=[\"SAMPLE\"])\n",
    "\n",
    "    # Creating initial opcode bi-gram dataframe\n",
    "    opcode_bi_gram_features = pd.DataFrame(columns=[\"SAMPLE\"])\n",
    "\n",
    "    # Creating initial opcode tri-gram dataframe\n",
    "    opcode_tri_gram_features = pd.DataFrame(columns=[\"SAMPLE\"])\n",
    "\n",
    "    # Creating the feature selector model\n",
    "    selector = SelectFromModel(RandomForestClassifier(n_estimators=1000))\n",
    "\n",
    "    # Iterating through all samples in the samples subdirectory\n",
    "    for root, dirs, files in os.walk(sample_dir):\n",
    "        for file in files:\n",
    "            # Collecting PE Header features from current sample\n",
    "            features = {}\n",
    "            try:\n",
    "                pe = pefile.PE(root+file)\n",
    "                features[\"SAMPLE\"] = (root+file)\n",
    "                features[\"CLASSIFICATION\"] = [1 if (\"malicious\" in root) else 0]\n",
    "                features[\"FILE_HEADER.MACHINE\"] = [pe.FILE_HEADER.Machine if (pe.FILE_HEADER != None) else 0]\n",
    "                features[\"FILE_HEADER.SIZEOFOPTIONALHEADER\"] = [pe.FILE_HEADER.SizeOfOptionalHeader if (pe.FILE_HEADER != None) else 0]\n",
    "                features[\"FILE_HEADER.CHARACTERISTICS\"] = [pe.FILE_HEADER.Characteristics if (pe.FILE_HEADER != None) else 0]\n",
    "                features[\"OPTIONAL_HEADER.IMAGEBASE\"] = [pe.OPTIONAL_HEADER.ImageBase if (pe.OPTIONAL_HEADER != None) else 0]\n",
    "                features[\"OPTIONAL_HEADER.MAJOROPERATINGSYSTEM\"] = [pe.OPTIONAL_HEADER.MajorOperatingSystemVersion if (pe.OPTIONAL_HEADER != None) else 0]\n",
    "                features[\"OPTIONAL_HEADER.MAJORSUBSYSTEMVERSION\"] = [pe.OPTIONAL_HEADER.MajorSubsystemVersion if (pe.OPTIONAL_HEADER != None) else 0]\n",
    "                features[\"OPTIONAL_HEADER.DLLCHARACTERISTICS\"] = [pe.OPTIONAL_HEADER.DllCharacteristics if (pe.OPTIONAL_HEADER != None) else 0]\n",
    "                features[\"OPTIONAL_HEADER.SUBSYSTEM\"] = [pe.OPTIONAL_HEADER.Subsystem if (pe.OPTIONAL_HEADER != None) else 0]\n",
    "                entropies = []\n",
    "                if (pe.OPTIONAL_HEADER != None):\n",
    "                    for section in pe.sections:\n",
    "                        entropies.append(section.get_entropy())\n",
    "                else:\n",
    "                    entropies.append(0)\n",
    "                features[\"PE_SECTIONS.MAXENTROPY\"] = max(entropies)\n",
    "                features[\"PE_SECTIONS.MINENTROPY\"] = min(entropies)\n",
    "                features[\"PE_SECTIONS.MEANENTROPY\"] = sum(entropies) / len(entropies)\n",
    "                # TODO: Compute the resource max and min entropy\n",
    "                if (pe.OPTIONAL_HEADER != None):\n",
    "                    for directory in pe.OPTIONAL_HEADER.DATA_DIRECTORY:\n",
    "                        features[\"DATA_DIRECTORY.\"+str(directory.name)] = [1 if ((directory.VirtualAddress != 0) and (directory.Size != 0)) else 0]\n",
    "                features[\"VS_VERSIONINFO.Length\"] = [pe.VS_VERSIONINFO[0].Length if (pe.VS_VERSIONINFO != None) else 0]\n",
    "                feature_df = feature_df.concat(features, ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            feature_df.filna(0)\n",
    "\n",
    "\n",
    "            # Collecting all byte bi-gram features in current sample\n",
    "            new_bi_grams = set()\n",
    "            try:\n",
    "                with open(root+file, \"rb\") as f:\n",
    "                    byte_bi_gram_features = byte_bi_gram_features.append({\"SAMPLE\": (root+file)}, ignore_index=True)\n",
    "                    cur_byte = f.read(1)\n",
    "                    prev_byte = None\n",
    "\n",
    "                    # While not the end of file\n",
    "                    while (cur_byte != b\"\"):\n",
    "                        # Creating the bi-gram if enough history exists\n",
    "                        if prev_byte != None:\n",
    "                            bi_gram = prev_byte.hex() + \" \" + cur_byte.hex()\n",
    "                            if not bi_gram in byte_bi_gram_features.columns:\n",
    "                                new_bi_grams.add(bi_gram)\n",
    "                            else:\n",
    "                                byte_bi_gram_features.loc[byte_bi_gram_features[\"SAMPLE\"]==(root+file) , [bi_gram]] = 1\n",
    "\n",
    "                        # Moving the sliding window\n",
    "                        prev_byte = cur_byte\n",
    "                        cur_byte = f.read(1)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            \n",
    "            # One-hot-encoding every sample with the combination of all encountered features\n",
    "            new_feature_array = []\n",
    "            for index, row in byte_bi_gram_features.iterrows():\n",
    "                if row[\"SAMPLE\"] == (root+file):\n",
    "                    new_feature_array.append([1 for x in new_bi_grams])\n",
    "                else:\n",
    "                    new_feature_array.append([0 for x in new_bi_grams])\n",
    "            byte_bi_gram_features = pd.concat([byte_bi_gram_features, pd.DataFrame(new_feature_array, columns=new_bi_grams)], axis=1)\n",
    "\n",
    "\n",
    "            # Collecting all opcode bi-gram and tri-gram features in current sample\n",
    "            # new_bi_grams = set()\n",
    "            # new_tri_grams = set()\n",
    "            # try:\n",
    "            #     with open(root+file, \"rb\") as f: # TODO: Convert this to open file in ASM\n",
    "            #         opcode_bi_gram_features = opcode_bi_gram_features.append({\"SAMPLE\": (root+file)}, ignore_index=True)\n",
    "            #         opcode_tri_gram_features = opcode_tri_gram_features.append({\"SAMPLE\": (root+file)}, ignore_index=True)\n",
    "            #         cur_opcode = f.read(1) # TODO: Convert this to find next opcode\n",
    "            #         prev_opcode1 = None\n",
    "            #         prev_opcode2 = None\n",
    "                    \n",
    "            #         # While not the end of file\n",
    "            #         while (cur_opcode != b\"\"):\n",
    "            #             # Creating the bi-gram if enough history exists\n",
    "            #             if prev_opcode1 != None:\n",
    "            #                 bi_gram = prev_opcode1.hex() + \" \" + cur_opcode.hex()\n",
    "            #                 if not bi_gram in opcode_bi_gram_features.columns:\n",
    "            #                     new_bi_grams.add(bi_gram)\n",
    "            #                 else:\n",
    "            #                     opcode_bi_gram_features.loc[opcode_bi_gram_features[\"SAMPLE\"]==(root+file), [bi_gram]] = 1\n",
    "\n",
    "            #             # Creating the tri-gram if enough history exists\n",
    "            #             if prev_opcode2 != None:\n",
    "            #                 tri_gram = prev_opcode2.hex() + \" \" + prev_opcode1.hex() + \" \" + cur_opcode.hex()\n",
    "            #                 if not tri_gram in opcode_tri_gram_features.columns:\n",
    "            #                     new_tri_grams.add(tri_gram)\n",
    "            #                 else:\n",
    "            #                     opcode_tri_gram_features.loc[opcode_tri_gram_features[\"SAMPLE\"]==(root+file), [tri_gram]] = 1\n",
    "\n",
    "            #             # Moving the sliding window\n",
    "            #             prev_opcode2 = prev_opcode1\n",
    "            #             prev_opcode1 = cur_opcode\n",
    "            #             cur_opcode = f.read(1) # TODO: Convert this to find next opcode\n",
    "            # except Exception as e:\n",
    "            #     print(e)\n",
    "\n",
    "            # # One-hot-encoding every sample with the combination of all encountered features\n",
    "            # new_feature_array = []\n",
    "            # for index, row in opcode_bi_gram_features.iterrows():\n",
    "            #     if row[\"SAMPLE\"] == (root+file):\n",
    "            #         new_feature_array.append([1 for x in new_bi_grams])\n",
    "            #     else:\n",
    "            #         new_feature_array.append([0 for x in new_bi_grams])\n",
    "            # opcode_bi_gram_features = pd.concat([opcode_bi_gram_features, pd.DataFrame(new_feature_array, columns=new_bi_grams)], axis=1)\n",
    "            # new_feature_array = []\n",
    "            # for index, row in opcode_tri_gram_features.iterrows():\n",
    "            #     if row[\"SAMPLE\"] == (root+file):\n",
    "            #         new_feature_array.append([1 for x in new_tri_grams])\n",
    "            #     else:\n",
    "            #         new_feature_array.append([0 for x in new_tri_grams])\n",
    "            # opcode_tri_gram_features = pd.concat([opcode_tri_gram_features, pd.DataFrame(new_feature_array, columns=new_tri_grams)], axis=1)\n",
    "\n",
    "    # Selecting top 200 byte bi-gram features\n",
    "    selector.fit(byte_bi_gram_features.iloc[:, 1:].to_numpy(), list(feature_df[\"CLASSIFICATION\"])) # TODO: Debug this\n",
    "    selected_byte_features = byte_bi_gram_features.columns[selector.get_support()]\n",
    "\n",
    "    # Selecting top 100 opcode bi-gram features\n",
    "    # selector.fit(opcode_bi_gram_features.iloc[:, 1:], list(feature_df[\"CLASSIFICATION\"])) # TODO: Copy the debugged version above\n",
    "    # selected_opcode_features_1 = opcode_bi_gram_features.columns[selector.get_support()]\n",
    "\n",
    "    # Selecting top 100 opcode tri-gram features\n",
    "    # selector.fit(opcode_tri_gram_features.iloc[:, 1:], list(feature_df[\"CLASSIFICATION\"])) # TODO: Copy the debugged version above\n",
    "    # selected_opcode_features_2 = opcode_tri_gram_features.columns[selector.get_support()]\n",
    "\n",
    "    # Creating final dataset with full feature matrix\n",
    "    #sample_df = pd.concat([feature_df, byte_bi_gram_features.loc[:, selected_byte_features], opcode_bi_gram_features.loc[:, selected_opcode_features_1], opcode_tri_gram_features.loc[:, selected_opcode_features_2]])\n",
    "    sample_df = pd.concat([feature_df, byte_bi_gram_features.loc[:, selected_byte_features]])\n",
    "\n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f7ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    print('Classification Report')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('Confusion Matrix')\n",
    "    confused = confusion_matrix(y_test, y_pred)\n",
    "    f = plt.figure(figsize=(15,15))\n",
    "    ax = f.add_subplot()\n",
    "    sns.heatmap(confused, annot=True, fmt='g', ax=ax)\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels([\"Malicious\", \"Benign\"])\n",
    "    ax.yaxis.set_ticklabels([\"Malicious\", \"Benign\"])\n",
    "    plt.show()\n",
    "    print('True Negative: ' + str(confused[0][0]))\n",
    "    print('True Positive: ' + str(confused[1][1]))\n",
    "    print('False Negative: ' + str(confused[0][1]))\n",
    "    print('False Positive: ' + str(confused[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f134fd9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'CLASSIFICATION'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'CLASSIFICATION'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9412\\3903014697.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# CREATING THE TRAINING DATASET\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_feature_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"CLASSIFICATION\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"CLASSIFICATION\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9412\\2959426979.py\u001b[0m in \u001b[0;36mcreate_feature_vectors\u001b[1;34m(sample_dir)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;31m# Selecting top 200 byte bi-gram features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_bi_gram_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"CLASSIFICATION\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# TODO: Debug this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[0mselected_byte_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbyte_bi_gram_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'CLASSIFICATION'"
     ]
    }
   ],
   "source": [
    "# CREATING THE TRAINING DATASET\n",
    "train = create_feature_vectors(train_dir)\n",
    "x_train = train.loc[:, train.columns != \"CLASSIFICATION\"]\n",
    "y_train = train[\"CLASSIFICATION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981bbad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE TESTING DATASET\n",
    "test = create_feature_vectors(test_dir)\n",
    "x_test = test.loc[:, test.columns != \"CLASSIFICATION\"]\n",
    "y_test = test[\"CLASSIFICATION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f23278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING AND TRAINING THE RFC CLASSIFIER\n",
    "model = RandomForestClassifier(n_estimators=1000).fit(x_train, y_train)\n",
    "evaluate_model(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE TRAINED MODEL\n",
    "pickle.dump(model, open(model_file, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
